\documentclass[x11names]{article}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{xcolor}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{array}
\usepackage[skins]{tcolorbox}
\usepackage[version=4]{mhchem}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{fourier}
\usepackage{xymtex}
\usepackage{textcomp}
\usepackage{eurosym}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage{pst-all}
\usepackage{pst-3dplot}
\usepackage{leftindex}
\usepackage{verbatim}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{hyperref}


\definecolor{myblue}{RGB}{224, 245, 255} 
\definecolor{myred}{RGB}{234, 222, 255}
\definecolor{myorange}{RGB}{255, 102, 0}

% box
\newtcolorbox{es}[2][]{%
  enhanced,colback=white,colframe=black,coltitle=black,
  sharp corners,boxrule=0.4pt,
  fonttitle=\itshape,
  attach boxed title to top left={yshift=-0.5\baselineskip-0.4pt,xshift=2mm},
  boxed title style={tile,size=minimal,left=0.5mm,right=0.5mm,
    colback=white,before upper=\strut},
  title=#2,#1
}

% definizioni
\newtcolorbox{blues}[2][]{%
  enhanced,colback=myblue,colframe=black,coltitle=black,
  sharp corners,boxrule=0.4pt,
  attach boxed title to top left={yshift=-0.5\baselineskip-0.4pt,xshift=2mm},
  boxed title style={tile,size=minimal,left=0.5mm,right=0.5mm,
    colback=myblue,before upper=\strut},
  title=#2,#1
}

% teoremi
\newtcolorbox{redes}[2][]{%
  enhanced,colback=myred,colframe=black,coltitle=black,
  sharp corners,boxrule=0.4pt,
  fonttitle=\itshape,
  attach boxed title to top left={yshift=-0.5\baselineskip-0.4pt,xshift=2mm},
  boxed title style={tile,size=minimal,left=0.5mm,right=0.5mm,
    colback=myred,before upper=\strut},
  title=#2,#1
}


% comandi per quadrati dimostrazioni
\newcommand*{\QEDA}{\null\nobreak\hfill\ensuremath{\blacksquare}}%
\newcommand*{\QEDB}{\null\nobreak\hfill\ensuremath{\square}}%


%% regole
\renewcommand*\contentsname{Indice}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{2}
\pgfplotsset{compat=1.15}

\usetikzlibrary{arrows}


\title{Geometria e Algebra lineare}
\author{Federico Cesari}
\date{}



%% DOCUMENTO


\begin{document}

\input{titlepage}
\tableofcontents
\newpage







%% %% %%
%% %% %%  Sistemi di equazioni lineari
%% %% %% 

\newpage
\section{Sistemi di equazioni lineari}
 Prima di parlare di sistemi definiamo cosa si intende per \textit{equazione lineare}: un'equazione lineare è un'uguaglianza del tipo  
 $$a_{1}x_{1} + a_{2}x_{2}... a_nx_n=b$$ 

 espressa nelle incognite $x_1$, $x_2$,... $x_n$. Un'equazinone di questo genere ha come soluzione una n-upla di numeri reali che sostituiti al posto delle incognite rende vera l'uguaglianza (la \textit{risoluzione} dell'equazione consiste nel trovare questa n-upla).

 \begin{es}{esempio}
     Definiamo un'equazione $*$ 
     $*:x_1 - x_2 + 2x_3 = 4 \qquad \textnormal{con} \qquad x_1 = x_2 - 2x_3 + 4$  \\
     L'insieme delle soluzioni di $*$ lo indichiamo con $S_{(*)}$ 
      $S_{(*)} = \Biggl\{\begin{pmatrix}x_2 - 2x_3 + 4 x_2 \\ x_3 \end{pmatrix}: x_2, x_3 \in \mathbb{R}\Biggl\} $ 
      In cui $x_2$ e $x_3$ sono i parametri liberi che variano.
 \end{es}

 \begin{es}{esempio}
     Utilizzando un'altra equazione $*$ 
     $*: 2x - 3y = 0$  \\
     L'insieme delle sue soluzioni sarà: 
     $S_{(*)} = \Biggl\{\begin{pmatrix}x \\ y \end{pmatrix}: x,y \in \mathbb{R}\Biggl\} $ 
     oppure tramite un parametro $t$ per il quale $\begin{cases}x=t\\y=\frac{2}{3}t ,\quad t \in \mathbb{R}\end{cases}$ 
      $S_{(*)} = \biggl\{(t,\frac{2}{3}t): t \in \mathbb{R}\biggl\} $
 \end{es}

 \vspace{2em}
 Definiamo invece un \textit{sistema lineare} di $r$ equazioni lineari in $n$ incognite $x_1, x_2 \dots x_n$ una struttura del tipo:

 $$ 
 {\begin{cases}a_{11}x_{1}+a_{12}x_{2}+\cdots +a_{1n}x_{n}=b_{1}\\a_{2,1}x_{1}+a_{22}x_{2}+ \cdots +a_{2n}x_{n}=b_{2}\\ \vdots \\ a_{r1}x_{1}+a_{r2}x_{2}+\cdots +a_{rn}x_{n}=b_{r}\end{cases}} 
 $$
 i coefficienti sono espressi nella forma $a_{ij}$ per agevolarne il riconoscimento all'interno del sistema. Il  pedice $i$ indica l'indice di riga, il pedice $j$ è l'indice di colonna. I termini noti $b$ presentandosi una sola volta per riga hanno solo l'indice di riga.
 Se i termini noti sono tutti nulli il sistema si dirà \textbf{\textit{omogeneo}}

 Diremo soluzione del sistema una n-upla di numeri reali $\begin{pmatrix}x_{1}\\ \vdots \\ x_{n}\end{pmatrix}$ che risolve ciascuna delle equazioni del sistema. Il sistema si dice \textbf{\textit{compatibile}} se ammette soluzioni (altrimenti \textbf{\textit{incompatibile}}).  Due sistemi sono \textbf{\textit{equivalenti}} se hanno lo stesso insieme di soluzioni. 
 
\begin{center}
\includesvg[scale=0.6]{figures/sistemi_piani}
\end{center}

\begin{center}
\fboxsep11pt
\colorbox{myred}{\begin{minipage}{5.75in}
\begin{redes}{}
%ROUCHE CAPELLI
\subsubsection{Teorema di Rouchè-Capelli}
Un sistema lineare $AX = B$  con $A \in \mathbb{R}^{r,n}, \quad X \in \mathbb{R}^{n,p}, \quad B \in \mathbb{R}^{r,p}$ è compatibile se e solo se il rango della matrice
dei coefficienti coincide con il rango della matrice completa. Se il sistema è compatibile, le soluzioni dipendono da $p \cdot \left(n-\text{rk}A\right)$ parametri liberi.
\[
	\text{rk}\left(A|\overline{b}\right) = \text{rk}\left(A\right) \quad \Rightarrow \quad \text{sistema compatibile}
\] 
\end{redes}
\end{minipage}}        
\end{center}

Poiché si opera solo sui coefficienti e non sulle incognite, i calcoli su essi risultano facilitati tramite l'utilizzo di tabelle (matrici). Un sistema quindi, nella sua forma matriciale (completa perché contiene anche i termini noti) il sistema si presenta così:

\[
\left(A|\overline{b}\right)=
\left(\begin{array}{ccc|c}
a_{1,1}&\cdots &a_{1,n}&b_{1} \\ \vdots &\ddots &\vdots  &\vdots \\ a_{r,1}&\cdots &a_{r,n}&b_{r}
\end{array}\right)
\] 

In questa forma la matrice è scomponibile e riscrivibile come il prodotto scalare tra il vettore dei coefficienti $\overline{a}$ e il vettore delle incognite $\overline{x}$:

\[
\begin{pmatrix}
a_{11}&a_{12}&\cdots &a_{1n}\\
a_{21}&a_{22}&\cdots &a_{2n}\\
\vdots &\vdots &\ddots &\vdots\\ 
a_{r1}&a_{r2}&\cdots &a_{rn}
\end{pmatrix}
\begin{pmatrix}x_{1}x_{2}\\
\vdots x_{n}
\end{pmatrix}=
\begin{pmatrix}
b_{1} \\
b_{2} \\
\vdots\\ 
b_{r}
\end{pmatrix}
\]

%OMOGENEO
\subsection{Sistemi omogenei}
Particolarità dei sistemi omogenei è il fatto che operando sulle righe, non si va ad alterare la colonna di zeri. Ogni sistema omogeneo ricade in due possibili scenari:

\begin{enumerate}
 	\item Ha solo la soluzione banale;
	 \item Ha altre infinite soluzioni oltre quella banale.
\end{enumerate}


\begin{center}
\fboxsep11pt
\colorbox{myred}{\begin{minipage}{5.75in}
\begin{redes}{}
\subsubsection{Teorema: parametri liberi}
Se un sistema lineare omogeneo ha $n$ incognite e nella sua forma ridotta la sua matrice completa ha  $\text{rank}A = n$ (nessuna riga nulla), allora
\[
\text{parametri liberi } = n - \text{rank}A
.\] 
\end{redes}
\end{minipage}}        
\end{center}

 

%SISTEMA OMOGENEO ASSOCIATO
\noindent
Se conosco una soluzione $x_0$ di $\Sigma$, sommandoci una qualsiasi soluzione del suo sistema omogeneo associato $\Sigma_{0}$ ottengo un'altra soluzione  di $\Sigma$.

Diciamo di avere un sistema molto semplice a un'equazione è il suo sistema omogeneo associato:
\[	
\Sigma : 3x - y = 5 \qquad \rightarrow y = 3x - 5 
\] 
\[
\Sigma_{0} = 3x - y = 0 \qquad \rightarrow y = 3x
.\] 
Le soluzioni di $\Sigma$ saranno:
\[
S\left(\Sigma\right): \left\{\left(\begin{array}{c} x  \\ 3x - 5\end{array}\right): x \in \mathbb{R}\right\}
\] 
di $\Sigma_0$ invece:
\[
S\left(\Sigma_0\right): \left\{x\left(\begin{array}{c} 1  \\ 3\end{array}\right): x \in \mathbb{R}\right\}
\] 
Le soluzioni di $\Sigma$ possono essere riscritte come una soluzione particolare (prendiamo quella con  $x=0$) sommata a tutte le soluzioni del sistema omogeneo associato:
 \[
 S\left(\Sigma\right): \left(\begin{array}{c} 0 \\ -5\end{array}\right) + x\cdot \left(\begin{array}{c} 1 \\ 3\end{array}\right)
\] 

\begin{es}{dimostrazione}
Iniziamo provando che la somma di due soluzioni di un sistema omogeneo rimane una soluzione:
\[
A \overline{x} = \overline{0} \qquad \left(\Sigma_0\right)
\] 
\[
A \overline{y} = \overline{0} \qquad \left(\Sigma_0\right)
\] 
\[
	\overline{x},\overline{y} \in S\left(\Sigma_0\right)
\] 
\[
A\left(\overline{x} + \overline{y}\right) = A\overline{x} + A\overline{y} = \overline{0} + \overline{0} = \overline{0} 
\] 
\[
\Rightarrow \quad \overline{x} + \overline{y} \in S\left(\Sigma_0\right)
\] 
Ovvio anche che  $\lambda \overline{x}$ o $\lambda \overline{y}$ entrambi $=0 \quad \forall \lambda \in \mathbb{R}$.
 
Proviamo poi che la somma di due soluzioni di $\Sigma$ \textit{non è mai} soluzione di $\Sigma$
\[
A \overline{x} = \overline{b} \qquad \left(\Sigma\right)
\] 
\[
A \overline{y} = \overline{b} \qquad \left(\Sigma\right)
\] 
\[
	\overline{x},\overline{y} \in S\left(\Sigma\right)
\] 
\[
A\left(\overline{x}+\overline{y}\right) = A\overline{x} + A\overline{y} = \overline{b} + \overline{b} = 2\overline{b}
\] 
Infine proviamo che una soluzione di $\Sigma + $  una qualsiasi soluzione di  $\Sigma_{0}$ è sempre una soluzione di $\Sigma$:
 \[
\overline{x} \in S\left(\Sigma\right), \quad \overline{y} \in S\left(\Sigma_0\right) \quad \Rightarrow \quad A\left(\overline{x} + \overline{y}\right) = A\overline{x} + A\overline{y} = \overline{b} + \overline{0}
\] 
\[
 = \overline{b} 
\]
\end{es}







%% %% %%
%% %% %%  Matrici
%% %% %% 

\newpage
\section{Matrici}

Definiamo una matrice di $r$ righe e $n$ colonne con $a_{ij} \in \mathbb{R}; \quad i = 1,\dots,r; j = 1,\dots,n$ definita nello spazio $\mathbb{R}^{rn}$ in questo modo:
$$
A=
\begin{bmatrix}a_{1,1}&a_{1,2}&\cdots &a_{1,n}\\a_{2,1}&a_{2,2}&\cdots &a_{2,n}\\ \vdots &\vdots &\ddots &\vdots \\a_{r,1}&a_{r,2}&\cdots &a_{r,n}\end{bmatrix}
$$
Si dicono matrici \textbf{quadrate} se hanno stesso numero di righe e di colonne ($\mathbb{R}^{nn}$), \textbf{diagonali} se tutti gli elementi sono zeri tranne quelli sulla diagonale maggiore (matrice \textit{unità} se la diagonale contiene solo $1$), \textbf{nulle} se tutti gli elementi sono zeri, \textbf{riga} se hanno una riga sola, \textbf{colonna} se hanno una colonna sola.

$$
\begin{bmatrix}1 & 1 & 0 \\ 0 & 1 & 2 \\ 0 & 3 & 1\end{bmatrix}
\hspace{1cm}
\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\ 0 & 0 & 1\end{bmatrix}
\hspace{1cm}
\begin{bmatrix}0 & 0 & 0\\0 & 0 & 0\\ 0 & 0 & 0\end{bmatrix}
\hspace{1cm}
\begin{bmatrix}1 & 2 & 3\end{bmatrix}
\hspace{1cm}
\begin{bmatrix}1\\ 2 \\ 3 \end{bmatrix}
$$

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: matrice ridotta}
    Una matrice si dice \textbf{ridotta} se in ogni sua riga \textit{non nulla} esiste un elemento sotto al quale ci sono solo zeri, questo elemento viene chiamato \textit{pivot}. Se una matrice è ridotta chiameremo \textit{rango della matrice} il numero di righe non nulle in essa: $rk(A) \leq min\{r,n\}$.
\end{blues}


%MATRICE A SCALA
\begin{blues}{Definizione: matrice a scala}
    Chiameremo \textit{primo pivot} il primo pivot nella prima riga partendo da sinistra. Una matrice si dice \textbf{a scala se \textit{è ridotta}} e se la riga $R_i$ è tutta fatta di zeri e, quindi, anche la riga $R_j$ per ogni $j>i$; ovvero: 
    $$
    R_i = \overline{0}\quad \Rightarrow \quad R_j = \overline{0} \quad \forall j>i
    $$
    Se la riga $R_i \neq \overline{0}$ il \textit{primo pivot} di $R_i$ è strettamente a destra del primo pivot di $R_{i-1}$.
\end{blues}
\end{minipage}}        
\end{center}


% rankA <= min(r,n)
\paragraph{Proposizione}
Il rango di una matrice $A$ non supera mai il minimo fra il numero di righe e di colonne.
\[
rk\left(a\right) \leq min\{r,n\} 
\]

\begin{es}{dimostrazione}
Sia $B'$ la riduzione a scala di $B$.
 
Sappiamo che $r_2$ di $B'$ comincia con almeno uno zero, $r_3$ con almeno 2 zeri, $r_4$ con almeno 3 zeri e così via. 

\[
\begin{bmatrix}
	a_{11} & a_{12} & \dots & a_{1n}  \\
	0     & a_{22}& \dots & a_{2n}  \\
	0 & 0 & a_{33} &  a_{3n}  \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{j1} & a_{j2} & \dots & a_{jn}  \\
	0 & 0 & 0 & 0 \\
	\vdots &\vdots &\vdots &\vdots \\
	0 & 0 & 0 & 0 
\end{bmatrix}
\] 
\[
\Rightarrow \quad R_{n+1} \text{è tutta di zeri}
\quad \Rightarrow \quad R_{j} \text{è tutta di zeri} \forall j \geq n + 1
\]
Se il rango è il numero di righe non nulle, e le righe dalla $n+1$ in poi sono tutte nulle, allora il rango è sicuramente minore o uguale a $n$. 
\\
\QEDB
\end{es}


%OPERAZIONI TRA MATRICI
\subsection{Operazioni tra matrici}
E' ammessa la somma tra matrici dello stesso ordine $\mathbb{R}^{rn}$ e sono ammesse la proprietà commutativa, associativa, esistenza dell'elemento neutro ed esistenza dell’opposto. Il prodotto $A\cdot B$ è ammesso se il numero di righe della prima è uguale al numero di colonne della seconda, ovvero se $A \in \mathbb{R}^{rn}$ e $B\in \mathbb{R}^{np}$. Per il prodotto sono valide la proprietà associativa, la distributiva del prodotto rispetto alla somma.

\subsubsection{Prodotto come combinazione lineare}
Un altro modo per descrivere il prodotto tra matrici è come \textit{combinazione lineare di vettori colonna}:
\[
\begin{bmatrix}
    -1 & 3 & 2 \\
    1 & 2 & -3 \\
    2 & 1 & -2 \
\end{bmatrix}
\begin{bmatrix}
2 \\ -1 \\ 3
\end{bmatrix}
=
\begin{bmatrix}
1 \\ -9  \\ -3
\end{bmatrix}
\] 

può anche essere scritto come:
\[
2 \begin{bmatrix}
-1 \\ 1 \\ 2
\end{bmatrix}
-1
\begin{bmatrix}
3 \\ 2   \\ 1 
\end{bmatrix}
+3
\begin{bmatrix}
2 \\ -3  \\ -2 
\end{bmatrix}
= 
\begin{bmatrix}
1 \\-9  \\-3 
\end{bmatrix}
\] 

\subsubsection{La trasposta di una matrice}
\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: matrice trasposta}
    Data una matrice $A \in \mathbb{R}^{r,n}$, si dice trasposta di A ($\leftindex^t{A}$) la matrice che si ottiene scambiando le righe con le colonne di $A$: Se $A = (a_{ij})$, $\leftindex^t{A} = (a_{ji})$.
$$
    A = \begin{bmatrix}
        1 & 2 & 3 \\
        0 & 1 & 2 
    \end{bmatrix}
    \qquad
    \leftindex^t{A} = \begin{bmatrix}
        1 & 0 \\
        2 & 1 \\
        3 & 2
    \end{bmatrix}
$$

\end{blues}
\end{minipage}}        
\end{center}

Proprietà delle matrici trasposte:
\begin{enumerate}
\item $\leftindex^t{\left(A+B\right)} = \leftindex^t{A} + \leftindex^t{B}$;
\item $\leftindex^t{(AB)} = \leftindex^t{B} \leftindex^t{A}$
\end{enumerate}

%INTERPRETAZIONE GEOMETRICA
\subsection*{Interpretazione geometrica delle matrici}
Approfondiamo il concetto di \textit{matrice} e il suo comportamento come \textit{spazio vettoriale}. Più in particolare vedremo il prodotto tra matrici come trasformazioni lineari dello spazio vettoriale (linearmente perché nessuna linea viene curvata e l'origine rimane fissata). Questa interpretazione di una matrice rende i conti più facili ed intuitivi. 

\noindent
Diciamo di avere due vettori giacenti sui due assi:

\begin{center}
\includesvg[scale=0.8]{figures/vettori_u}
\end{center}

\noindent
Diciamo ora di avere una matrice $A$ che descrive dove questi due vettori cadono a seguito della trasformazione da essa descritta; la matrice $A$ basta per descrivere dove cadrà ogni vettore (x,y). 
$$
x=\begin{bmatrix}
    1 & 3 \\
    -2 & 0 
\end{bmatrix}
$$
$$
\begin{bmatrix}
   1 & 3 \\
   -2 & 0 
\end{bmatrix}
\begin{bmatrix}
    x \\
    y
\end{bmatrix}
= x
\begin{bmatrix}
    1\\-2
\end{bmatrix}
+ y
\begin{bmatrix}
    3 \\  0
\end{bmatrix}
$$
\textit{Se, come abbiamo detto prima, le linee non vengono deformate, il vettore che nel primo grafico sarebbe stato (1,1), nel secondo è intuitivo pensare che ora sia (4,-2); Il prodotto tra le matrici lo conferma}.

\begin{center}
\includesvg[scale=0.8]{figures/vettori_du}
\end{center}

\noindent
Possiamo arrivare alla conclusione che ogni matrice può essere interpretata come una trasformazione dello spazio, a prescindere dall'ordine della matrice.
\paragraph*{Prodotto come composizione di trasformazioni}
Se applichiamo più trasformazioni consecutive, quindi tramite più matrici, interpretiamo la composizione di queste trasformazioni come il prodotto tra le matrici.
$$
\begin{bmatrix}
    \textcolor{Brown2}{1} & \textcolor{Brown2}{1} \\
    \textcolor{Brown2}{0} & \textcolor{Brown2}{1} 
\end{bmatrix}
\begin{bmatrix}
    \textcolor{SteelBlue3}{0} & \textcolor{SteelBlue3}{-1} \\
    \textcolor{SteelBlue3}{1} & \textcolor{SteelBlue3}{0} 
\end{bmatrix}
=
\begin{bmatrix}
    1 & -1 \\
    1 & 0 
\end{bmatrix}
$$

\noindent
L'ordine di composizione va letto da destra verso sinistra: viene eseguita prima la \textcolor{SteelBlue3}{blu}, poi la \textcolor{Brown2}{rossa} (cosa tipica della notazione delle funzioni: $f(g(x))$).

\noindent
Pensando in questi termini, prodotto come composizione di trasformazioni, comprendiamo perché $AB \neq BA$; e la proprietà associativa diventa chiara e logica: $A(BC) = (AB)C$ in quanto l'ordine delle trasformazioni rimane invariato.



%DETERMINANTE
\newpage
\subsection{Determinante}
\subsection*{Interpretazione geometrica del determinante}
Il determinante di una matrice geometricamente rappresenta il fattore di "stretching" di un'area, o volume tridimensionale o n-dimensionale (qualunque cosa sia).
$$
A = \begin{bmatrix}
    2 & 1 \\
    1 & 2
\end{bmatrix}
\rightarrow
detA = 3
$$

\begin{center}
\includegraphics[scale=0.3]{figures/determ.png}
\end{center}
\subsubsection{Calcolo del determinante}
Il determinante di una matrice è una funzione $det: \mathbb{R}^{nn} \Longrightarrow \mathbb{R}$ che verifica queste due proprietà:
\begin{enumerate}
    \item Se $a $ è un numero reale, ossia una matrice di ordine uno quadrata, allora $det(a) = a$ .
    \item   Se A = $\begin{bmatrix}a_{11} & a_{12}\\a_{21} & a_{22}\end{bmatrix}$ allora $det(A) = a_{11}a_{22} - a_{12}a_{21}$
\end{enumerate}
Dallo sviluppo del caso due notiamo che compaiono due addendi ciascuno dei quali è il prodotto di due fattori. I due fattori nei due prodotti di iniziano entrambi uno con un $1$ e uno con un $2$ per poi seguire con le \textit{permutazioni di $(1,2): (1,2),(2,1)$.} Perché il secondo prodotto ha un $-$ davanti? Il segno è dettato dalla \textit{parità} della permutazione, ovvero: per arrivare alla coppia $(1,2)$ si devono attuare degli scambi, se il numero di scambi è pari, il segno non cambia, se gli scambi sono dispari, il segno cambia.


\begin{es}{esempio}
\begin{align*}
A=
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\ 
a_{21} & a_{22} & a_{23}  \\
a_{31} &a_{32} & a_{33} 
\end{bmatrix} \rightarrow
det(A) &= a_{11} a_{22} a_{33} + a_{13} a_{21} a_{32} \textcolor{Brown2}{-} a_{11} a_{23} a_{32} \textcolor{Brown2}{-} a_{13} a_{22} a_{31} \textcolor{Brown2}{-} a_{12} a_{21} a_{33} \\
&= \sum_{\sigma} \epsilon (\sigma)a_{1\sigma(1)}a_{2\sigma (2)}a_{3\sigma(3)}
\end{align*}
\end{es}

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: determinante}
Il determinante di una matrice quadrata $A = (a_{ij})$ di ordine $n$ è dato da: 
$$
\sum_{\sigma} \epsilon (\sigma)a_{1\sigma(1)}a_{2\sigma (2)}\dots a_{n\sigma(n)}
$$
dove $\sigma$ è una qualsiasi permutazione dei numeri $1,2,\dots,n$ e $\epsilon(\sigma)$ è il suo segno. 
\end{blues}
\end{minipage}}        
\end{center}

\subsubsection{Come cambia il determinante dopo le 3 mosse?}
\begin{enumerate}
    \item $det^t(A) = det(A)$;
    \item Se $A'$ si ottiene scambiando due righe o due colonne di $A$, allora $det(A') = -det(A)$;
    \item Se moltiplico una riga per un numero reale $\lambda$ allora $det(A^1) = \lambda^n det(A)$;
    \item Se addiziono a una riga un multiplo di un'altra riga il determinante non cambia;
    \item Una matrice con due righe o colonne uguali ha determinante nullo;
    \begin{enumerate}
        \item data la matrice $A \in \mathbb{R}^{n,n}$, $rk(A) = n \longleftrightarrow det(A) \neq 0$
        \item analogamente $rk(A) < n \longleftrightarrow det(A) = 0$
    \end{enumerate}
            
    \item $det(A+B) \neq det(A) + det(B)\quad \forall A,B \in \mathbb{R}^{n,n}$
\item \colorbox{myred}{\begin{minipage}{3.5in}\textbf{Teorema di Binet}: $det(AB) = det(A) det(B) \quad \forall A,B \in \mathbb{R}^{n,n}$;\end{minipage}}
    \item $det(A^{-1}) = (det(A))^{-1}$;
\end{enumerate}

\begin{es}{dimostrazione determinante (2)}
È conseguenza della definizione di determinante e del fatto che lo scambio di due righe comporta il cambiamento di segno di ciascuna permutazione. Per esempio, nel caso della matrice quadrata di ordine 2 si ha:    

$$
\begin{vmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
\end{vmatrix}
= a_{11}a_{22} - a_{12}a_{21}
$$
Se scambio due righe:
$$
\begin{vmatrix}
    a_{21} & a_{22} \\
    a_{11} & a_{12} 
\end{vmatrix}
= - a_{22}a_{11} + a_{21}a_{12}
$$

\QEDB
\end{es}
\begin{es}{dimostrazione determinante (5a)}
    Operando su una matrice $A$ e la rendo $A'$ a scala triangolare superiore. So allora che $det(A')=\lambda \cdot det(A)$ e $det(A) \neq 0 \Leftrightarrow det(A') \neq 0$.
    Quindi $a'_{ij} \neq 0 \quad \forall j \in \{1,\dots,n\}$.

    Cioè $A'$ ha $n$ righe non nulle, quindi $rk(A') = n$.

\QEDB
\end{es}
\begin{es}{dimostrazione determinante (8)} 
Per il teorema di Binet:
$$
AA^{-1} = I \qquad A^{-1} = \frac{I}{A} \qquad det(A^{-1}) = det\left(\frac{I}{A} \right)= \frac{1}{det(A)}
$$

\QEDB
\end{es}





\vspace{1em}
\noindent
\paragraph{osservazioni su matrici inverse}
Se il determinante di una matrice è uguale a zero, significa che la matrice è non invertibile. Questo perché il determinante descrive, anche se non esplicitamente, il numero di soluzioni del sistema di equazioni associato.

\noindent
Il determinante è infatti strettamente legato al \textbf{rango}: se il rango, ovvero il numero di righe non nulle, di una matrice $A \in \mathbb{R}^{r,n}$ è minore di $n$ sappiamo che il determinante vale $0$ e che di conseguenza il sistema\textit{ non può avere una singola soluzione}. Infatti se la matrice ha una riga nulla o più, le soluzioni saranno infinite e legate a uno o più parametri liberi.

\noindent
Se il sistema associato alla matrice $A$ non ha una singola soluzione è chiaro come non possa esistere una matrice $A'$ inversa che soddisfi:
$$
AA^{-1}= I \quad A^{-1} = \frac{I}{A}
$$
L'equazione ha infatti una sola soluzione se e solo se $A$ fosse unicamente definita.

% \subsection{Equazioni matriciali}
% Per equazioni matriciali si intende un'equazione le cui incognite sono matrici del tipo:
% $$
% AX = B
% $$
% $$
% \begin{pmatrix}a_{11}&a_{12}&\cdots &a_{1n}a_{21}&a_{22}&\cdots &a_{2n}\vdots &\vdots &\ddots &\vdots a_{r1}&a_{r2}&\cdots &a_{rn}\end{pmatrix}
% \begin{pmatrix}x_{11}&x_{12}&\cdots &x_{1n}x_{21}&x_{22}&\cdots &x_{2n}\vdots &\vdots &\ddots &\vdots x_{r1}&x_{r2}&\cdots &x_{rn}\end{pmatrix} = 
% \begin{pmatrix}b_{11}&b_{12}&\cdots &b_{1n}b_{21}&b_{22}&\cdots &b_{2n}\vdots &\vdots &\ddots &\vdots b_{r1}&b_{r2}&\cdots &b_{rn}\end{pmatrix}
% $$
% con $A \in \mathbb{R}^{r,n}$, $X \in \mathbb{R}^{n,p}$, $B \in \mathbb{R}^{r,p}$.
% La risoluzione si riconduce a quella di un sitema lineare del tipo:
% $$
% {\begin{cases}a_{11}x_{11}+a_{12}x_{21}+\cdots +a_{1n}x_{n1}=b_{11}a_{11}x_{12}+a_{22}x_{22}+\cdots +a_{2n}x_{n}=b_{2}\vdots a_{11}x_{1p}+a_{12}x_{2p}+\cdots +a_{1n}x_{np}=b_{1p}\end{cases}}
% $$

\subsection{Teoremi di Laplace}
I teoremi di Laplace permettono di semplificare i conti nel calcolo del determinante di una matrice $n \times n$  a conti di un determinante $(n-1) \times (n-1)$. I conti vengono semplificati perché si procede a scegliere un elemento $a_{ij}$ nella matrice (vedremo perché di solito è uno in una riga o colonna con tanti zeri) , "nascondendo" tutti gli elementi della riga e colonna del nostro candidato e andremo a calcolare il determinante della matrice "rimanente", questo determinante lo chiameremo \textbf{minore} di $a_{ij}$  e lo indichiamo con $M_{ij}$.
Ora serve definire il \textbf{cofattore}; il cofattore di $a_{ij}$ è il numero $A_{ij}$ definito dalla formula:
$$
A_{ij}  = (-1)^{ij}\cdot M_{ij}
$$
Il fattore $(-1)^{ij}$ da segno positivo o negativo se la posizione di $a_{ij}$ è pari o dispari ($a_{11}$ è pari, $a_{12}$ è dispari...). 

\subsubsection{Primo Teorema di Laplace}
Fissata la riga i-esima, il determinante di una matrice quadrata $A \in \mathbb{R}^{n,n}$ è dato dalla somma di tutti i prodotti tra gli elementi della riga e i rispettivi cofattori (questo metodo funziona anche con le colonne) :
$$
\text{det}(A) = \sum_{j=1}^{n}a_{ij}A_{ij}
$$
\subsubsection{Secondo Teorema di Laplace}
In una matrice quadrata $A \in \mathbb{R}^{n,n}$ la somma dei prodotti tra gli elementi di una riga (o colonna) e i  cofattori \textit{di una riga parallela} è zero:
\begin{align*}
  0 &= \sum_{k=1}^{n}a_{ik}A_{jk}  \\
    &= \sum_{h=1}^{n}a_{hi}A_{hj} \qquad i \neq j
\end{align*}



\begin{es}{verifica}
    % Abbiamo visto nel primo teorema che ciascuno dei cofattori  $A_{ij}$ "\textit{non vede}" la riga i-esima, quindi:
    È conseguenza evidente della proprietà $(2)$ del determinante secondo la quale \textit{se scambio due righe o colonne a una matrice allora il suo determinante cambia di segno.} Si puó interpretare come lo sviluppo del determinante di un matrice in cui, nel primo caso, la riga j-esima coincide con la riga i-esima e nel secondo caso, la colonna j-esima coincide con la riga i-esima.
    $$
    A = \begin{bmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{bmatrix} 
    $$
    Se per esempio scegliamo di moltiplicare gli elementi della prima riga per i complementi della seconda abbiamo:
    $$
    a\begin{vmatrix}
        b & c \\
        h & i
    \end{vmatrix}
    -b\begin{vmatrix}
        a & c \\
        g & i
    \end{vmatrix}
    +c\begin{vmatrix}
        a & b \\
        g & h
    \end{vmatrix}
    $$
    $$
    = abi - ach - bai + bcg + cah - cbg = 0
    $$
\end{es}


%MATRICI INVERSE
\subsection{Matrici inverse}
\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: matrice invertibile}
    Una matrice $A \in \mathbb{R}^{n,n}$ si dice invertibile se $\exists$ una matrice $X$ tale che $AX = XA = I$.
\end{blues}
\end{minipage}}        
\end{center}
\subsubsection*{Proprietà generali delle matrici inverse}
\begin{enumerate}
    \item Se esiste una matrice inversa allora questa è univocamente determinata e la chiamo $A^{-1}$;
    \item $(AB)^{-1} = B^{-1}A^{-1}$;
    \item Se $A,B$ sono invertibili non è detto che lo sia $A+B$;
    \item $(\leftindex ^t{A})^{-1} = \leftindex ^t(A^{-1})$.
\end{enumerate}



\begin{es}{dimostrazione (1)}
Supponiamo che $X$ e $X'$ soddisfino:
$$
XA = I = AX 
$$
$$
X' A = I = AX'
$$
$$
XAX = \begin{array}{c}
       (XA)X' = IX' = X' \\
       X(AX') = XI = X
    \end{array}
    \rightarrow
    XA = AX'
$$
Abbiamo dimostrato che se esiste una $X$ inversa a sinistra per $A$ ed esiste una $X' $ inversa a destra per $A$, allora $X=X'$ e quindi $A$ è invertibile e X è la sua inversa. 

\QEDB
\end{es}
\begin{es}{dimostrazione (2)}
Controllo se la candidata ad inversa $B^{-1}A^{-1}$ soddisfa le proprietà richieste:
$$
(B^{-1}A^{-1})AB = B^{-1}(A^{-1}A)B = B^{-1}IB = B^{-1}B = I
$$
$$
AB(B^{-1}A^{-1})= \dots = I
$$
\QEDB
\end{es}

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema: matrice inversa}
			Sia A una matrice quadrata di ordine $n$, se $\text{det}(A) \neq 0$ allora esiste l’inversa di $A$ ed è:
			\[
			A^{-1} = \frac{1}{\text{det}(A)}\text{adj}(A)
			\]	
			\end{redes}
	\end{minipage}}        
\end{center}


\begin{es}{dimostrazione}
Dai teoremi di Laplace so che:
\[
	\sum_{j=1}^n a_{ij}A_{kj} = \left\{\begin{array}{c} \text{det}A \quad \text{se} \quad i = k \\ 0 \quad \text{se}\quad i \neq k \end{array} \right
.\] 
ovvero che la somma dei prodotti tra tutti gli elementi di una riga di $A$ e i rispettivi cofattori è uguale o a $0$ o al determinante di A.

Ovvero che il prodotto tra la matrice $A$ e la trasposta della matrice dei cofattori di A ($\text{adj}A$) si può scrivere come:
\[
A \cdot \text{adj}A = \begin{bmatrix}
	\text{det}A & 0 & \dots & 0 \\
	0 & \text{det}A & \dots & 0\\
	\vdots & 0 &  \ddots & 0 \\

\end{bmatrix}
= \text{det}A \cdot I
\]
Possiamo notare quindi che dopo le opportune operazioni ci si riconduce alla formula iniziale.
\\
\QEDB
\end{es}



\subsubsection{Calcolo della matrice inversa 1}
Il primo metodo consiste nello svolgimento di un'equazione matriciale:
$$
AX = I
$$
Che si risolve come:
$$
(A|I) = \left[\begin{array}{cccc|cccc}
    a_{11} & a_{12} & \dots & a_{1n} & 1 & 0& \dots & 0 \\
    a_{21} & a_{22} & \dots & a_{2n} & 0 & 1& \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots &  \vdots &  \vdots& \ddots & \vdots \\
    a_{n1} & a_{n2} & \dots & a_{nn} & 0 & 0& \dots & 1 \\
\end{array}\right]
$$

\subsubsection{Calcolo della matrice inversa 2}
Possiamo calcolare la matrice inversa anche a partire dalla nozione di determinante dopo aver parlato dei teoremi di Laplace.

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: matrice aggiunta}
Si dice \textbf{matrice aggiunta} di $A$ la trasposta della matrice contentente i \textit{cofattori } di $A$:
$$
\text{Adj}(A)_{ij} = [A_{ij}]
$$
Per esempio:
$$
A=\begin{bmatrix}
    1 &2&3 \\
-1&2&5 \\
0 &1&2
\end{bmatrix}
\qquad
\text{Adj}(A) = \begin{bmatrix}
    -1 &-1 &4 \\
2 &2 &-8 \\
-1 &-1& 4
\end{bmatrix}
$$
\end{blues}
\end{minipage}}        
\end{center}

I teoremi di Laplace più la matrice adiacente ci permettono di  determinare in modo esplicito la formula dell'inversa.


%CRAMER

\subsection{Teorema di Cramer}
Subito dopo aver descritto un nuovo modo per calcolare la matrice inversa vediamo come può tornare utile nella risoluzione di sistemi lineari con $n$ incognite e $n$ equazioni.
$$
A\overline{x} = \overline{b}
$$
$$
\overline{x} = A^{-1}\overline{b}
$$
$$
\overline{x} = \frac{1}{\text{det}(A)}\text{adj}(A) \cdot \overline{b}
$$
$$
\overline{x} = \begin{bmatrix}
    x_1 \\ x_2 \\ \vdots\\  x_n
\end{bmatrix} = \frac{1}{\text{det}(A)}
\begin{bmatrix}
    A_{11} & A_{12} & \dots & A_{1n} \\
    A_{21} & A_{22} & \dots & A_{2n}  \\
    \vdots & \vdots & \ddots & \vdots  \\
    A_{n1} & A_{n2} & \dots & A_{nn}
\end{bmatrix}
\begin{bmatrix}
    b_1 \\ b_2 \\ \vdots\\  b_n
\end{bmatrix}
= \frac{1}{\text{det}(A)}
\begin{bmatrix} 
    A_{11}b_1 & A_{12}b_2 & \dots & A_{1n}b_n  \\
    A_{21}b_1 & A_{22}b_2 & \dots & A_{2n}b_n  \\
    \vdots & \vdots & \ddots & \vdots \\
    A_{n1}b_1 & A_{n2}b_2 & \dots & A_{nn}b_n 
\end{bmatrix}
$$

da cui: 
\begin{align*}
    x_i &= \frac{1}{\text{det}(A)}(b_1 A_{2i} + b_2  A_{2i} + \dots + b_nA{ni})  \\
   &= \frac{1}{\text{det}(A)} \text{det}(\overline{a}_1 | \overline{a}_2 \dots |\overline{b}| \dots |\overline{a}_n)
\end{align*}



%% %% %%
%% %% %%  PRODOTTO SCALARE e VETTORIALE
%% %% %% 

\newpage
\section{Prodotto scalare e vettoriale}
Prima di parlare di prodotto scalare è necessario introdurre due concetti fondamentali:

\begin{enumerate}
	\item \textbf{Lunghezza} di un vettore che d'ora in poi chiameremo \textit{norma};
	\item \textbf{Angolo} tra due vettori.
\end{enumerate}

\begin{center}
\includegraphics[scale=0.2]{figures/norma.png}
\end{center}

La norma del vettore è definita dalla formula:
\[
\|\bar{v}\| = \sqrt{v_1^2 + v_2^2 + \dots + v_{n}^2}
.\] 

Vedremo come saranno di estrema importanza i vettori di norma 1, o vettori unitari. Per esempio in $V_3$ i vettori unitari sono $i,j,k$. In generale per \textit{normalizzare} un vettore basta dividerlo per la sua lunghezza, quindi per la sua norma:
\[
\bar{u} = \frac{\bar{v}}{\|\bar{v}\|}
.\] 


\begin{center}
\includegraphics[scale=0.25]{figures/angoli.png}
\end{center}
Per quanto riguarda l'angolo tra due vettori invece prenderemo in considerazione solo la parte compresa tra $0$ e $\pi$.


Ora che sappiamo cosa sono norma di un vettore e angolo tra vettori possiamo parlare di \textbf{prodotto scalare}. E' infatti necessario introdurre un'operazione moltiplicativa "utile" per vettori in $\mathbb{R}^2$ e in $\mathbb{R}^3$.

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: prodotto scalare}
Il prodotto scalare (in $V_3$ ) $x\cdot y$ di due vettori $x$ e $y$ in $V_3$ è la funzione:
\[
\cdot : \quad V_3 \times V_3 \quad \longrightarrow \quad \mathbb{R}
.\] 
così definita:
\[
\bar{x }\cdot \bar{y}  = \|\bar{x}\| \|\bar{y}\| \cos{\hat{xy}}
.\] 

\end{blues}
\end{minipage}}        
\end{center}

Dalla definizione troviamo altre due espressioni di norma e angolo (notare come ora il concetto di angolo sia esteso a tutto $\mathbb{R}^n$):
\[
\|\bar{v}\| = \sqrt{\bar{v}\cdot \bar{v}} \qquad \qquad \cos{\theta} = \frac{\bar{u}\cdot \bar{v}}{\|\bar{u}\|\|\bar{v}\|}
.\]

\subsection{Proprietà del prodotto scalare}
\begin{itemize}
	\item $\bar{x} \cdot \bar{y}$ = $\bar{y} \cdot \bar{x} \quad \forall \bar{x},\bar{y} \in V_3$;
	\item $\left(\bar{x} + \bar{y}\right)\cdot \bar{z} = \bar{x}\cdot \bar{z} + \bar{y}\cdot \bar{z}$;
	\item $\left(\lambda \bar{x}\right)\cdot \bar{z} = \lambda \bar{x} \cdot \bar{z }= \bar{x }\cdot \left(\lambda \bar{z}\right)$;
	\item $\bar{x}\cdot \bar{x} \geq 0 \qquad = 0 \Longleftrightarrow \bar{x} = 0$.
\end{itemize}

\subsection{Interpretazione geometrica del prodotto scalare}
Il prodotto scalare $\|a\|\|u\|\cos{\theta}$ non è altro che il prodotto della lunghezza di uno dei due vettori ($\|a\|$) per la proiezione ortogonale con segno dell'altro sul primo($\|u\|\cos{\theta}$).

\begin{center}
\includegraphics[scale=0.25]{figures/proj.png}
\end{center}


\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
			\subsubsection{Teorema: vettore proiezione ortogonale}
			Dati due vettori $u$ e $a$ non nulli il vettore proiezione ortogonale di $u$ su $a$ è:
			\[
			\bar{p} = \frac{\bar{u}\cdot\bar{ a}}{\|\bar{a}\|^2}\bar{a}
			.\] 	
			\end{redes}
	\end{minipage}}        
\end{center}



\begin{es}{dimostrazione}
Il mio obiettivo è quello di scrivere la proiezione di $u$ su $a$ in questa forma:
\[
\bar{p} = * \frac{\bar{a}}{\|\bar{a}\|}
.\] 
dove "*" indica la lunghezza della proiezione.
Guardando la figura in alto sappiamo che la proiezione $\|p\| = \|u\|\cos{\theta}$. Quindi:
\[
\bar{p} = \|\bar{u}\|\cos{\theta} \frac{\bar{a}}{\|\bar{a}\|} 
.\] 
Per elimiare il coseno di theta risaliamo alla formula di prodotto scalare:
\[
\bar{u} \cdot \bar{a} = \|\bar{u}\|\|\bar{a}\|\cos{\theta} \quad \longrightarrow \quad \|\bar{u}\|\cos{\theta} = \frac{\bar{u}\cdot \bar{a}}{\|\bar{a}\|}
.\] 
Quindi:
\[
\bar{p }= \frac{\bar{u}\cdot \bar{a}}{\|\bar{a}\|^2}\bar{a}
.\] 

\QEDB
\end{es}

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
			\subsubsection{Teorema di Pitagora generalizzato}
			Dati $u$ e $v$ vettori ortogonali tra loro in $\mathbb{R}^n$ con prodotto standard, allora
			\[
			\|u+v\|^2 = \|u\|^2 + \|v\|^2
			.\] 
			\textit{la dimostrazione è molto semplice, il termine $2u\cdot v$ vale zero.}		
			\end{redes}
	\end{minipage}}        
\end{center}





\subsection{Prodotto vettoriale}

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: prodotto vettoriale}
Il prodotto vettoriale (in $V_3$ ) $x\cdot y$ di due vettori $x$ e $y$ in $V_3$ è la funzione:
\[
\wedge : \quad V_3 \times V_3, \quad \longrightarrow \quad \left(x,y\right) \mapsto x \wedge y
.\] 
così definita:
\[
x \wedge y  = \|x\wedge y\| \|x\| \|y\| \sin{\hat{xy}}
.\] 

\end{blues}
\end{minipage}}        
\end{center}
Il verso del vettore risultante dal prodotto vettoriale ha il verso che segue la \textit{regola della mano destra}.


\noindent
Possiamo anche scrivere il prodotto scalare tramite lo sviluppo di determinanti in questo modo:
\[
u \wedge v = 
\begin{vmatrix}
     i& j & k \\
     u_1& u_2 & u_3 \\
     v_1& v_2 & v_3 
\end{vmatrix}
\] 

\[
	=
\left(
\begin{vmatrix}
    u_2 & u_3  \\
     v_2& v_3  
\end{vmatrix}i,
- \begin{vmatrix}
    u_1 &u_3 \\
     v_1&v_3   
\end{vmatrix}j, +
\begin{vmatrix}
    u_1 & u_2 \\
     v_1 &  v_2   
\end{vmatrix}k
\right)
.\] 

\subsubsection{Proprietà del prodotto vettoriale}
\begin{itemize}
	\item $u \wedge v = -\left(v\wedge u\right)$ ;
	\item $u \wedge \left(v+w\right) = u\wedge v + u \wedge w$;
	\item $k\left(u\wedge v\right) = ku \wedge v = u \wedge kv$ ;
	\item $u \wedge u = 0$
\end{itemize}

\subsubsection{Interpretazione geometrica del prodotto vettoriale}
Dati $u$ e $v$ vettori in uno spazio tridimensionale, dall'identità di Lagrange sappaiamo che:
\begin{align*}
	\|u\wedge v\|^2 &= \|u\|^2\|v\|^2 - \left(u\cdot v\right)^2 \\
			&= - \|u\|^2\|v\|^2\cos{\theta}^2 \\
			&= \|u\|^2\|v\|^2 \left(1-\cos{\theta}^2\right) \\
			&= \|u\|^2\|v\|^2 \sin{\theta}^2 \\
\end{align*}
\noindent
Da questo possiamo notare che il prodotto vettoriale può essere inteso anche come area del parallelogramma che ha come lati i due vettori. 

\begin{center}
\includegraphics[scale=0.25]{figures/uedge.png}
\end{center}

\subsection{Prodotto misto}

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Prodotto misto}
Dati due vettori $u$ e $v$, allora
\[
u \cdot \left(v \wedge w\right)
.\] 
è chiamato prodotto misto di $u$ $v$ e  $w$. 

Geometricamente il prodotto misto rappresenta $6$ volte il volume del tetraedro formato dai tre vettori
\end{blues}
\end{minipage}}        
\end{center}

\noindent
Il prodotto misto di tre vettori può essere risolto tramite determinante come 
\[
\bar{u} \cdot \left(\bar{v} \wedge \bar{w}\right) = \begin{vmatrix}
	u_{1} & u_{2} & u_{3} \\
	v_{1} & v_{2} & v_{3} \\
	w_{1} & w_{2} & w_{3} \\
\end{vmatrix}
\]
e vale 
\[
\mathcal{B} = \left\{\bar{u},\bar{v},\bar{w}\right\} \text{ destrorsa }\Longleftrightarrow \bar{u} \cdot \left(\bar{v} \wedge \bar{w}\right) > 0< \Longleftrightarrow \text{ det}M^{\mathcal{B}}_{\mathcal{B}'} > 0
\]
\[
\forall \mathcal{B}' \text{ base ortonormale destrorsa}
\]
Si ha infatti che 

\[
 \text{ det}M^{\mathcal{B}}_{\mathcal{B}'} = \text{ det}\left[\begin{array}{c}
 	\leftindex^{t}{[\bar{u}]}_{\mathcal{B}'} \\
 	\leftindex^{t}{[\bar{v}]}_{\mathcal{B}'} \\
 	\leftindex^{t}{[\bar{w}]}_{\mathcal{B}'} \\
 \end{array}\right] = \begin{vmatrix}
 u_{1} & u_{2} & u_{3} \\
 v_{1} & v_{2} & v_{3} \\
 w_{1} & w_{2} & w_{3} \\
 \end{vmatrix}
\]



%% %% %%
%% %% %%  SPAZI VETTORIALI
%% %% %% 

\newpage
\section{Spazi Vettoriali}
\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Spazio Vettoriale}
    Si definisce \textbf{spazio vettoriale} sul campo dei numeri reali $\mathbb{R}$ un certo insieme $V$ nel quale sono definite le seguenti operazioni:
    \begin{enumerate}
        \item somma $+$: $V \times V \longrightarrow V$.
        \item prodotto $\cdot$: $\mathbb{R} \times V \longrightarrow V$.
        \end{enumerate}
	Un gruppo $\left(V,\times \right)$ si dice \textit{commutativo} (o Abeliano) se $v \times y = y \times x \quad \forall x,y \in V$
\end{blues}
\end{minipage}}        
\end{center}


\subsubsection{Proprietà della somma}
\begin{enumerate}
    \item commutativa: $x + y = y + x, \quad \forall x,y \in V$
    \item associativa: $(x+y)+z = x + (y+z),\quad \forall x,y \in V$
    \item esistenza dell'elemento neutro: $\exists 0 \in V : 0 + x = x +0, \quad \forall x,y \in V$
    \item esistenza dell'opposto: $\forall x \in V \exists -x \in V : x + (-x) = (-x) +x = 0$
\end{enumerate}
\subsubsection{Proprietà del prodotto}
\begin{enumerate}
    \item (diciamo) distributiva: $\lambda(x+y) = \lambda x + \lambda y,\quad \forall x,y \in V, \forall \lambda \in \mathbb{R}$
    \item $(\lambda + \mu)\cdot \bar{x} = \lambda \bar{x} + \mu \bar{x}, \quad \forall x,y \in V, \forall \lambda \in \mathbb{R}$
    \item $(\lambda \cdot \mu)\bar{x} = \lambda(\mu \cdot \bar{x}),\quad \forall x,y \in V$
    \item $1 \cdot \bar{x} = \bar{x}, \quad \forall x,y \in V$
\end{enumerate}



\subsection{Sottospazi vettoriali}
\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: sottospazio vettoriale}
Sia $V$ uno spazio vettoriale reale, $W \subseteq V $ è un sottospazio vettoriale di $V$ se $W$ è uno spazio vettoriale rispetto alle stesse operazioni di $V$, quindi rispetto alle operazioni di \textit{somma} e \textit{prodotto}.

\begin{itemize}
	\item Se ho 2 elementi $\bar{x},\bar{y}\in W \quad \Rightarrow \quad \bar{x} + \bar{y} \in W$;
	\item Se ho 2 elementi $\bar{x} \in W, \lambda \in \mathbb{R} \quad \Rightarrow \quad \lambda\bar{x} \in W$.
\end{itemize}

\end{blues}
\end{minipage}}        
\end{center}

\begin{center}
\includegraphics[scale=0.25]{figures/subspace.png}
\end{center}
In figura vediamo come $u$ e $v$ siano contenuti in $W$ ma la loro somma no.

	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Spazio vettoriale Euclideo}
				Uno spazio vettoriale $V$ si dice euclideo se rispetta le proprietà di:
				\begin{enumerate}
					\item \textbf{Simmetria}: $\bar{x} \cdot \bar{y} = \bar{y} \cdot \bar{x}$;
					\item \textbf{Bilinearità}: $\left(\bar{x} + \bar{y}\right) \cdot \bar{z} = \bar{x} \cdot \bar{z} + \bar{y} \cdot \bar{y}$, $\qquad \left(\lambda \bar{x}\right) \cdot \bar{z} = \bar{x} \cdot \left(\lambda \bar{z}\right)$;
					\item \textbf{Definito positivo}: $\bar{x} \cdot \bar{x} \geq 0$
				\end{enumerate}
			\end{blues}
	\end{minipage}}       
\end{center}

\begin{es}{Esempio fondamentale di sottospazio vettoriale}
L'insieme delle soluzioni di un sistema lineare omogeneo di $m$ equazioni in $n$ incognite è un sottospazio vettoriale di $\mathbb{R}^n$. L'insieme delle soluzioni di 
\[
AX = \bar{0} \qquad A \in \mathbb{R}^{m,n}, X \in \mathbb{R}^{n,1}, \bar{0} \in \mathbb{R}^{m,1}
\] 
è detto \textit{nullspace} e coincide con l'insieme

\[
N\left(A\right) = \left\{ X \in \mathbb{R}^n | AX = \bar{0}  \right\}
\]

Dati $X_1$ e $X_2$ $\in N\left(A\right)$ si deve dimostrare che 
\[
\lambda X_1 + \mu X_2 \in N\left(A\right) \quad \forall \lambda,\mu \in \mathbb{R}
\] 
che sviluppando
\[
A\left(\lambda X_1 + \mu X_2\right) = \lambda AX_1 + \mu AX_2 = \bar{0}
.\] 
\end{es}


\subsubsection{$+$ Somma di due sottospazi}
La somma di due sottospazi è il più piccolo sottospazio contenente l'unione dei due si esprime come l'insieme di tutti i vettori ottenuti dalla somma di vettori appartenenti ai sottospazi sommati.
\[
W_1 + W_2  = \left\{\bar{x} \in V: \quad \bar{x} = \bar{y}+\bar{z}  \qquad y \in W_1, \quad z \in W_2\right\}
.\] 

Quindi $W_1 + W_2$ contiene $W_1$ e contiene $W_2$ e $W_1 + W_2$ è sottospazio.
\begin{es}{dimostrazione}
Possiamo dire che è sottospazio se la somma tra ogni vettore ricade in esso così come il prodotto tra ogni vettore e uno scalare.

Prendiamo due vettori $\bar{x}$ e $\bar{y}$ entrambi $\in W_1 + W_2$:
\[
\bar{x} = \bar{x}_{1} + \bar{x}_{2} \quad \bar{x}_1 \in W_1, \quad \bar{x}_{2} \in W_2
.\] 
\[
\bar{y} = \bar{y}_{1} + \bar{y}_{2} \quad \bar{y}_1 \in W_1, \quad \bar{y}_{2} \in W_2
.\] 
\begin{align*}
	\left(+\right)\Rightarrow \quad \bar{x} + \bar{y} &= \bar{x}_{1} + \bar{x}_{2} + \bar{y}_{1} + \bar{y}_{2} \\
						      &= \left(\bar{x}_{1} + \bar{y}_{1}\right) + \left(\bar{x}_{2}+\bar{y}_{2}\right) \quad \Rightarrow \quad \in W_1 + W_2 
\end{align*}
\begin{align*}
	\left(\cdot\right) \Rightarrow \quad \lambda\left(\bar{x}+\bar{y}\right) &= \lambda \bar{x}_{1} + \lambda \bar{x}_{2} + \lambda \bar{y}_{1} + \lambda \bar{y}_{2} \\
						      &= \lambda\left(\bar{x}_{1} + \bar{y}_{1}\right) + \lambda\left(\bar{x}_{2}+\bar{y}_{2}\right) \quad \Rightarrow \quad \in W_1 + W_2
\end{align*}  \QEDB
\end{es}


\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Somma diretta}
$\left(V,+,\cdot\right)$ spazio vettoriale, $W_1,W_2 \leq V$. Diciamo che la somma $W_1+W_2$ è una somma diretta se ogni $\bar{x} \in W_1+W_2$ si scrive in modo unico come $\bar{x}_1 + \bar{x}_2$ con $\bar{x}_1 \in W_1$ e $\bar{x}_2 \in W_2 $.

La somma verrà scritta come:
\[
W_1 \oplus W_2 = V
.\] 
\end{blues}
\end{minipage}}        
\end{center}

\paragraph{Proposizione}
In $V$ spazio vettoriale, $W_1,W_2 \leq V$. Allora:
\[
W_1 \quad \text{e} \quad W_2 \quad \text{sono in somma diretta} \quad \Longleftrightarrow \quad W_1 \cap W_2 = \{\bar{0}\} 
.\] 

\begin{es}{dimostrazione}
Dimostro la contronominale:


$\Leftarrow \qquad \text{se esiste un $\bar{x}$ con almeno 2 scomposizioni} \quad \Rightarrow \quad \exists \bar{z} \in W_1 \cap W_2,\quad \bar{z} \neq \bar{0}$.\\


Prendo allora tale $\bar{x}$ che si scompone in due coordinate x e in due y:
\[
\bar{x}_{1},\bar{y}_{1} \in W_{1} \qquad \bar{x}_{2},\bar{y}_{2} \in W_{2}
\]
\[
\bar{x}_{1} + \bar{x}_{2} = \bar{x} = \bar{y}_{1} + \bar{y}_{2}, \quad \bar{x}_{1} \neq \bar{x}_{2}, \quad \bar{y}_{1} \neq \bar{y}_{2}
.\] 
\[
\bar{x}_{1} + \bar{x}_{2} = \bar{y}_{1} + \bar{y}_{2}
.\] 
\[
\bar{y}_{1} - \bar{x}_{1} = \bar{x}_{2} - \bar{y}_{2} \quad = \bar{z}
.\] 
Il vettore $\bar{z}$ è contenuto sia in $W_1$ sia in $W_2$, quindi $W_1 \cap W_2 \neq \bar{0}$. \\


Contronominale: se $W_1 \cap W_2 \neq \{\bar{0}\} \quad \Rightarrow \quad \text{non ho unicità di scrittura}$
\[
\Rightarrow \quad \text{Se} \quad \bar{z} \in W_1 \cap W_2, \quad\bar{z} \neq \bar{0}
.\] 
\[
x \in W_1 + W_2
.\] 
\[
\bar{x} = \bar{x}_{1} \textcolor{red}{+\bar{z}} + \bar{x}_{2} \textcolor{red}{-\bar{z}}
.\]  \QEDB
\end{es}


\subsubsection{$\cap$ Intersezione di due sottospazi}
L'intersezione di due sottospazi vettoriali $W_1$ e $W_2$ contiene tutti i vettori contenuti sia in $W_1$ sia in $W_2$.

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\paragraph{Teorema: l'intersezione è sottospazio}
				Immediata conseguenza delle definizioni di sottospazio vettoriale e di intersezione. Se $W_1$ e $W_2$ sono sottospazi allora lo deve essere anche $W_1 \cap W_2$.
					
			\end{redes}
	\end{minipage}}        
\end{center}



\subsection{Combinazione lineare}

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: combinazione lineare}
Dato $V$ spazio vettoriale, $\bar{v}_{1},\dots,\bar{v}_{n} \in V$, una composizione lineare di $\bar{v}_{1},\dots,\bar{v}_{n}$ è una strutura del tipo $\lambda_{1}\bar{v}_{1},\dots,\lambda_{n}\bar{v}_{n}$ con ogni $\lambda \in \mathbb{R}$.
\end{blues}
\end{minipage}}        
\end{center}
Ogni sottospazio possiamo dire essere generato da combinazioni lineari dei vettori che lo generano. 


\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema: somma più piccolo sottospazio}
				Se $S=\{w_1,w_2,\dots,w_{r}\}$ è un insieme di vettori non nullo contenuto in $V$, allora:  \\
				L'insieme  $W = \mathscr{L}\left(\left(w_1\right),\left(w_2\right),\dots,\left(w_{r}\right)\right)$, è il \textbf{più piccolo} sottospazio di V che contiene tutti i vettori di $S$. \\
				In questo caso si dice che  $W$  \textbf{è generato} da $S$.
				
			\end{redes}
	\end{minipage}}        
\end{center}



\begin{center}
\includegraphics[scale=0.3]{figures/span.png}
\end{center}

\noindent
E' importante riconoscere che gli insiemi di generatori \textit{non sono unici}. Per esempio qualsiasi vettore non nullo sulla linea in figura sarebbe generatore  di $v$. Sono generatori tutte le combinazioni lineari di un insieme di generatori.

\subsection{Indipendenza lineare}
Diciamo di avere uno spazio $xy$ con vettori standard $i$ e $j$. Ogni vettore in $xy$ può essere espresso in modo unico come combinazione lineare di $i$ e $j$. Supponiamo ora di introdurre una terza coordinata $w$ a 45 gradi tra gli assi $x$ e $y$. 

Questo terzo asse risulta essere totalmente superfluo poiché lui stesso può essere espresso come combinazione lineare di $i$ e $j$ per cui non aiuta a descrivere nessun vettore sul piano.


\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Vettori linearmente indipendenti}
Dato un insieme di vettori $V=\{\bar{v}_1,\bar{v}_2,\dots,\bar{v}_{r}\}$, questo  è detto \textit{linearmente indipendente} se nessun vettore in $V$ può essere espresso come combinazione lineare di uno degli altri. Quindi se e solo se:
\[
k_1\bar{v}_1 + k_2\bar{v}_2 + \dots + k_r\bar{<}_r = 0
.\] 
è risolto solo da $k_1 =k_2= \dots = k_r =0$.

Un insieme di vettori linearmente indipendenti si dice \textbf{libero}.
\end{blues}
\end{minipage}}        
\end{center}

\begin{center}
\includegraphics[scale=0.22]{figures/linind.png}
\end{center}
$$
\begin{array}{ccc}
	l.d.\qquad \qquad \qquad \qquad \qquad \qquad& l.i. & \qquad \qquad \qquad \qquad \qquad \qquad l.i. 
\end{array}
$$ 
\begin{center}
\includegraphics[scale=0.22]{figures/linind2.png}
\end{center}
$$
\begin{array}{ccc}
	l.d.\qquad \qquad \qquad \qquad \qquad \qquad& l.d. & \qquad \qquad \qquad \qquad \qquad \qquad l.i. 
\end{array}
$$

\subsection{Basi}
Ora che sappiamo cosa sono un insieme di generatori e un insieme di vettori linearmente indipendenti possiamo definire il concetto di base.


\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Base}
Viene chiamata base di $V$ un insieme di vettori che \textbf{genera} $V$ e al contempo è linearmente indipendente.
\end{blues}
\end{minipage}}        
\end{center}

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema: unicità di scrittura}
				Data una base $\mathscr{B} = \{\bar{v}_1 + \bar{v}_2 + \dots + \bar{v}_{n}\}$ ogni vettore $v \in V$ può essere espresso nella forma
				\[
				\bar{v} = c_1\bar{v}_1 + c_2\bar{v}_2 + \dots + c_{n}\bar{v}_{n}
				\]
				in un solo modo.
			\end{redes}
	\end{minipage}}        
\end{center}



\begin{es}{dimostrazione}
Diciamo che esista un'altra combinazione lineare che esprime $v$; abbiamo:
\[
\bar{v} = c_1\bar{v}_1 + c_2\bar{v}_2 + \dots + c_{n}\bar{v}_{n}
\]
\[
\bar{v} = k_1\bar{v}_1 + k_2\bar{v}_2 + \dots + \bar{k}_{n}\bar{v}_{n}
\]
allora una scrittura meno l'altra deve essere uguale zero
\[
c_1\bar{v}_1 + c_2\bar{v}_2 + \dots + c_{n}\bar{v}_{n} - k_1\bar{v}_1 + k_2\bar{v}_2 + \dots + k_{n}\bar{v}_{n} = 0
\] 
\[
	\left(c_1-k_1\right)\bar{v}_1 + \left(c_2-k_2\right)\bar{v}_2 + \dots + \left(c_{n} - k_{n}\right)\bar{v}_{n} = 0
\] 
da ciò risulta che
\[
c_1 - k_1 = c_2 - k_2 = \dots = c_{n} -k_{n} = 0
\] 
\[
c_i = k_{i} \quad \forall i \in {1,\dots,n}
.\] \QEDB
\end{es}


\subsubsection{Lemma di Steinitz}
				Dato $V$ finitamente generato e una sua base $\mathcal{B} = {\bar{v}_{1},\dots,\bar{v}_{n}}$:
				\[
				V = \mathcal{L}(\bar{v}_{1},\dots,\bar{v}_{n}) 
				\]
				Prendiamo un insieme libero $\mathcal{I}$ contenuto in V:
				\[
				\mathcal{I} = \{\bar{w}_{1},\dots,\bar{w}_{p}\} \quad \text{libero } \subseteq V \quad  \quad \Longrightarrow \quad p \leq n
				\]

\begin{es}{dimostrazione}
	Supponendo $\bar{w}_{1} \in V$ non nullo.
	\[
	\bar{w}_{1}  = \lambda_1 \bar{v}_{1} + \dots + \lambda_n \bar{v}_{n}
	\]
	visto che il vettore è non nullo posso dire senza perdita di generalità che uno dei $\lambda$ è diverso da zero. Divido allora tutto per un lambda, diciamo $\lambda_1$ così da esplicitare $\bar{v}_{1}$:
	\[
	\bar{v}_{1} = \frac{1}{\lambda_1}\bar{w}_{1} - \frac{\lambda_{2}}{\lambda_{1}}\bar{v_{2}} - \dots - \frac{\lambda_{n}}{\lambda_{1}}\bar{v}_{n}
	\] 
	\[
	\Longrightarrow V = \mathcal{L}\left(\bar{w}_{1},\bar{v}_{2},\dots,\bar{v_{n}}\right)
	\]
	Posso ripetere l'operazione con $\bar{w}_{2}$:
	\[
	\bar{w}_{2} = \lambda_{1}\bar{w}_{1} + \lambda_2\bar{v}_{2} + \dots + \lambda_n \bar{v}_{n}
	\]
	So che è impossibile che tutti i $\lambda_{2}, \dots , \lambda_{n}$ siano nulli (se no si avrebbe $\bar{w}_{2} = \lambda_{1} \bar{w}_{1}$ e $\mathcal{I}$ sarebbe libero):
	\[
	\bar{v}_{2} = \frac{1}{\lambda_2}\bar{w}_{2} - \frac{\lambda_{1}}{\lambda_2}\bar{w}_{1} - \frac{\lambda_{3}}{\lambda_{2}}\bar{v_{3}} - \dots - \frac{\lambda_{n}}{\lambda_{2}}\bar{v}_{n}
	\]
	\[
	\Longrightarrow V =  \mathcal{L}\left(\bar{w}_{1},\bar{w}_{2},\bar{v}_{3},\dots,\bar{v_{n}}\right)
	\]
	Iterando il processo posso concludere in due modi:
	\begin{enumerate}
		\item Metto tutti i $\bar{w}_{j}$, possibile solo se $p \leq n$;
		\item Arrivo a scrivere una base di $V$ del tipo $\mathcal{L}\left(\bar{w}_{1},\dots,\bar{w_{p}}\right)$ con $p > n$. Ciò è possibile se 
		\[
		\bar{w}_{n+1} \in V \quad \Longrightarrow \quad \bar{w}_{n+1}  = \lambda_{1}\bar{w}_{1} + \dots + \lambda_{n}\bar{w_{n}}
		\]
		\textbf{assurdo} poiché $\{\bar{w}_{1} + \dots + \bar{w}_{n+1}\} \subseteq \mathcal{I}$ che è insieme libero.
	\end{enumerate} \QEDB
\end{es}


				\subsubsection{Formula di Grassman}
				Siano $W_{1}$ e $W_{2}$ due sottospazi vettoriali di uno spazio vettoriale $V$, allora:
				\[
				\text{dim}\left(W_{1} + W_{2}\right) = \text{dim}W_{1} + \text{dim}W_{2} - \text{dim}\left(W_{1} \cap W_{2}\right)
				\]

\begin{es}{dimostrazione}
	Iniziamo prendendo una base per $W_{1} \cap W_{2}$: $\mathcal{B} = \left(\bar{a}_{1},\dots,\bar{a}_{k}\right)$.
	
	Costruisco una base per ognuno dei sottospazi:
	\[
	\mathcal{C} = \left(\bar{a}_{1},\dots,\bar{a}_{k},\bar{b}_{k+1},\dots,\bar{b}_{l}\right) \qquad \text{base di } W_{1}
	\]
	\[
	\mathcal{D} = \left(\bar{a}_{1},\dots,\bar{a}_{k},\bar{c}_{k+1},\dots,\bar{c}_{p}\right) \qquad \text{base di } W_{2}
	\]
	La tesi consiste nel dimostrare che 
	\[
	\mathcal{E} = \left(\bar{a}_{1},\dots,\bar{a}_{k},\bar{b}_{k+1},\dots,\bar{b}_{l},\bar{c}_{k+1},\dots,\bar{c}_{p}\right)
	\]
	è una base di $W_{1} + W_{2}$.
	
	So di per certo che $\mathcal{E}$ è un insieme di generatori (ho messo solo generatori), devo però dimostrare che è \textit{libero}, ovvero che:
	\[
		\alpha_{1} \bar{a}_{1} + \dots + \alpha_{k} \bar{a}_{k} + \beta_{k+1} \bar{b}_{k+1} + \dots + \beta_{l} \bar{b}_{l} + \gamma_{k+1} \bar{c}_{k+1} + \dots + \gamma_{p} \bar{c}_{p} = 0 \quad \Longleftrightarrow \quad \text{ tutti gli }\alpha, \beta, \gamma = 0
	\]
	
	Portandone una parte a destra dell'uguale troviamo
	\[
	\alpha_{1} \bar{a}_{1} + \dots + \alpha_{k} \bar{a}_{k} + \beta_{k+1} \bar{b}_{k+1} + \dots + \beta_{l} \bar{b}_{l}  = -\gamma_{k+1} \bar{c}_{k+1} - \dots - \gamma_{p} \bar{c}_{p}  \mathbf{= \bar{v}} 
	\]
	quindi si ha che 
	\[
	\bar{v} \in W_{1} \cap W_{2}
	\]
\end{es}
\begin{es}{}
	e quindi è riscrivibile come combinazione lineare dei vettori delle basi $\mathcal{C}$ e $\mathcal{D}$:
	\[
	\bar{v} = 	\alpha_{1} \bar{a}_{1} + \dots + \alpha_{k} \bar{a}_{k} + \beta_{k+1} \bar{b}_{k+1} + \dots + \beta_{l} \bar{b}_{l}\quad \Longleftrightarrow \quad \text{ tutti gli }\alpha, \beta = 0
	\]
	\[
	\bar{v} = 	\alpha_{1} \bar{a}_{1} + \dots + \alpha_{k} \bar{a}_{k} + \gamma_{k+1} \bar{c}_{k+1} + \dots + \gamma_{p} \bar{c}_{p}\quad \Longleftrightarrow \quad \text{ tutti gli }\alpha, \gamma = 0
	\]
	Abbiamo dimostrato quindi che per poter scrivere $\bar{v}$ in questo modo è necessario che tutti gli $\alpha,\beta,\gamma$ devono valere zero e che di conseguenza l'insieme $\mathcal{E}$ è composto da \textit{generatori linearmente indipendenti}. Quindi posso dire che 
	\[
	\mathcal{E} \text{ è una base di } V \text{ e che } \quad \text{dim}(W_{1} + W_{2}) = l + p - k
	\]
	 \QEDB
\end{es}

\subsection{Cambiamento di base}
In moltissimi casi risulta più comodo esprimere un vettore rispetto a una base diversa da quella di partenza. Se cambiamo da una base $\mathscr{B}$ a una $\mathscr{B}'$ come saranno correlate le componenti $\left[v\right]_{B}$ e $\left[v\right]_{B'}$?

\noindent
Le vecchie coordinate sono legate alle nuove dalla seguente equazione:

\[
\left[v\right]_{B} = M_{B}^{B'} \left[v\right]_{B'}
\] 
$M_{B}^{B'}$ è detta  \textbf{matrice del cambiamento di base} e ha nelle colonne i vettori della base di arrivo rispetto la base di partenza. Quindi la matrice di cambiamento di base da $\mathscr{B} = {v_1,\dots,v_{n}}$ a $\mathscr{B}' = {e_{1},\dots,e_{n}}$ sarà:


\[
M_{B}^{B'} =
\left[\begin{array}{c|c|c}
     \left[v'_1\right]_{B}
&  \dots & \left[v'_{n}\right]_{B}
\end{array}
\right]
\] 

\begin{es}{per convincersene}
Diciamo di avere base di partenza e base di arrivo:
\[
\mathscr{B} = \{\bar{v}_{1},\bar{v}_{2}\} \qquad \mathscr{B}' = \{\bar{v}'_{1},\bar{v}'_{2}\}
\]
Sappiamo che i vettori della nuova base possono essere scritti come combinazione lineare dei ettori della vecchia base: a partire da $\mathcal{B}$ definiamo i vettori di $\mathcal{B}'$.
\[
v'_{1} = \textcolor{myorange}{a}\bar{v}_{1} + \textcolor{myorange}{b}\bar{v}_{2}
\]
\[
v'_{2} = \textcolor{myorange}{c}\bar{v}_{1} + \textcolor{myorange}{d}\bar{v}_{2}
\]
La base di arrivo può essere riscritta come $ \mathcal{B}' = \left\{(\textcolor{myorange}{a}\bar{v}_{1} + \textcolor{myorange}{b}\bar{v}_{2}),(\textcolor{myorange}{c}\bar{v}_{1} + \textcolor{myorange}{d}\bar{v}_{2}) \right\}$.

Un vettore rispetto a $\mathcal{B}'$ si può riscrivere come combinazione lineare dei vettori della base:

\[
\left[v\right]_{\mathcal{B}'}  = \left[\begin{array}{c}
		k_{1} \\ k_{2}
\end{array} \right]
\]

\begin{align*}
	v &= \textcolor{red}{k_{1}}(\textcolor{myorange}{a}\bar{v}_{1} + \textcolor{myorange}{b}\bar{v}_{2}) + \textcolor{red}{k_{2}}(\textcolor{myorange}{c}\bar{v}_{1} + \textcolor{myorange}{d}\bar{v}_{2}) \\
	&= \left(\textcolor{red}{k_{1}}\textcolor{myorange}{a} + \textcolor{red}{k_{2}}\textcolor{myorange}{c}\right)\bar{v}_{1} + 
	\left(\textcolor{red}{k_{1}}\textcolor{myorange}{b} + \textcolor{red}{k_{2}}\textcolor{myorange}{d} \right)\bar{v}_{2} 
\end{align*}
\end{es}
\begin{es}
	
Adesso possiamo riscrivere il vettore rispetto alla base di partenza $\mathcal{B}$
\begin{align*}
[v]_{\mathcal{B}} &= \left[\begin{array}{cc}
	\textcolor{red}{k_{1}}\textcolor{myorange}{a} & \textcolor{red}{k_{2}}\textcolor{myorange}{c} \\
	\textcolor{red}{k_{1}}\textcolor{myorange}{b} & \textcolor{red}{k_{2}}\textcolor{myorange}{d}
\end{array}\right] \\
	&=\left[\begin{array}{cc}
		\textcolor{myorange}{a} & \textcolor{myorange}{c} \\
		\textcolor{myorange}{b} &\textcolor{myorange}{d}
	\end{array}\right]
	\left[
	\begin{array}{c}
		\textcolor{red}{k_{1}}\\
		\textcolor{red}{k_{2}}
	\end{array}\right]
\end{align*}
Abbiamo ritrovato la forma

\[
[v]_{\mathcal{B}} =  M_{B}^{B'} [v]_{\mathcal{B}'}
\] 

\end{es}

\subsubsection{Metodo per trovare la matrice di cambiamento di base}
Per trovare la matrice di cambiamento di base si può utilizzare un metodo simile a quello impiegato per trovare la matrice inversa: si scrive la matrice orlata con a sinistra la base $\mathscr{B}$ e a destra la base $\mathscr{B}'$. Quindi si riduce per righe fino a quando la matrice non ha la forma
\[
	\left[\begin{array}{c|c}
			I & M_{B}^{B'}
	\end{array}\right]
\] 

\subsection{Basi ortonormali}
Basi ortonormali sono particolarmente perché il prodotto scalare si scrive in modo semplice e ciò rende particolarmente facile anche trovare le componenti dei vettori espressi rispetto a basi standard:
\[
\mathcal{B} = \{\bar{i},\bar{j},\bar{k}\} \qquad \bar{x} = x_{1}\bar{i} + x_{2}\bar{j} +x_{3}\bar{k}
\]
allora la componente $i$ di $\bar{x}$ la trovo sviluppano il prodotto scalare 
\[
\bar{i} \cdot \bar{x} = \mathbf{x_{1}} + 0x_{2} + 0x_{3}
\]

\noindent
Chiamiamo \textbf{matrici ortogonali} matrici che hanno nelle colonne componenti di vettori \textit{ortonormali} fra di loro. Quindi si hanno matrici con colonne di norma 1 e ortogonali fra di loro con il prodotto standard. Sono matrici ortogonali le matrici di cambiamento di base tra basi ortonormali:
\[
P = M^{\mathcal{B}'}_{\mathcal{B}} \qquad \mathcal{B},\mathcal{B}' \text{ ortonormali}
\]
Si noti che 
\[
\left(\leftindex^{t}{P}\cdot P\right)_{ij} = (\text{riga } i \text{ di } \leftindex^{t}{P}) \cdot (\text{colonna } i \text{ di } P) = \bar{v}_{i} \cdot \bar{v}_{j} = \begin{cases}
	1 \quad i = j\\
	0 \quad i \neq j
\end{cases}
\] 

Quindi
\[
\leftindex^{t}{P}P = I \qquad \leftindex^{t}{P} = P^{-1}
\]

\subsubsection{Algoritmo di ortogonalizzazione di Gram-Schmidt }
Questo algoritmo ci permette di ottenere un insieme di vettori \textbf{ortogonali} a partire da un generico insieme di vettori linearmente indipendenti in uno spazio vettoriale dotato di un prodotto scalare \textit{definito positivo}.

\noindent
Diciamo di avere tre vettori
\[
\bar{v}_{1} \quad \bar{v}_{2} \quad \bar{v}_{3}
\]
Si prende il primo vettore e si \textbf{normalizza}:
\[
\bar{u}_{1} = \frac{\bar{v}_{1}}{\|\bar{v}_{1}\|}
\]
Poi si rende ortogonale $\bar{v}_{2}$
\[
\bar{v}_{2}' = \bar{v}_{2} - \left(\bar{v}_{2} \cdot \bar{u}_{1}\right)\bar{u}_{1}
\]
si normalizza $\bar{v}_{2}'$
\[
\bar{u}_{2} = \frac{\bar{v}_{2}'}{\|\bar{v}_{2}'\|}
\]
Si itera con gli altri vettori
\[
\bar{v}_{3}' = \bar{v}_{3} - \left(\bar{v}_{3} \cdot \bar{u}_{1}\right)\bar{u}_{1} - \left(\bar{v}_{3} \cdot \bar{u}_{2}\right)\bar{u}_{2}
\]
\[
\bar{u}_{3} = \frac{\bar{v}_{3}'}{\|\bar{v}_{3}'\|}
\]


\subsection{Spazio delle righe, delle colonne, Nullspace}
Prendiamo in esame la matrice
\[
A=
\begin{bmatrix}a_{1,1}&a_{1,2}&\cdots &a_{1,n}\\a_{2,1}&a_{2,2}&\cdots &a_{2,n}\\ \vdots &\vdots &\ddots &\vdots \\a_{r,1}&a_{r,2}&\cdots &a_{r,n}\end{bmatrix}
\] 

Chiamiamo spazio delle colonne l'insieme dei vettori colonna nella matrice $A$ e spazio delle righe l'insieme dei vettori riga in  $A$. Il \textbf{nullspace} di $A$ invece (introdotto nel capitolo sui sistemi lineari) ricordiamo esseere la solzione dell'equazione  $AX = 0$.

\paragraph{C(A) e nullspace}

Che relazione intercorre tra lo spazio delle colonne e il nullspace?
Scriviamo l'equazione $Ax = b$ per colonne:
\[
	Ax = x_1c_1 + x_2c_{2} + \dots + x_{n}c_{n} = b
\] 
da questa scrittura notiamo che \textit{$b$ può essere scritto come combinazione lineare delle colonne di $A$}. Allora $b \in C\left(A\right)$, $b$ fa parte dello spazio delle colonne di $A$. 

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
								\subsubsection{Teorema del rango}
				Data $f: V \to W$, con $\text{dim}V = n$, allora possiamo affermare in modo equivalente che:
				\[
				\text{dim}Ker(f) + \text{dim}Im(f) = \text{dim}(V)
				\]
				\[
				N(A) + C(A) = n
				\]
				
				
			\end{redes}
	\end{minipage}}        
\end{center}

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
								\subsubsection{Teorema di nullità + rango}
				Il teorema afferma che
				\[
				R(A) \oplus^\perp N(A) = \mathbb{R}^n
				\]
				ovvero che lo spazio delle righe è in somma diretta e ortogonale con il \textit{nullspace} della matrice A.
				
				Questo ci mostra anche:
				\[
				\text{dim}N(A) + \text{rk}A = n
				\]
				Ovvero che la dimensione nel nullspace equivale al numero di righe nulle e quindi anche al numero di parametri liberi.
				
			\end{redes}
	\end{minipage}}        
\end{center}

\begin{es}{dimostrazione}
	Sappiamo che il nullspace è l'insieme dei vettori $\bar{x}$ che soddisfano
	\[
	A\bar{x} = 0
	\]
	Se riscriviamo $A$ evidenziandone le \textbf{righe} notiamo che
	\[
	\left(\begin{array}{c}
		R_{1} \\
		\vdots \\
		R_{r}
	\end{array}\right)\bar{x} = \bar{0}
	\quad \to \quad R_{j} \cdot \bar{x} = \bar{0} 
	\]
	Quindi $\bar{x}$ è ortogonale allo spazio delle righe (rispetto al prodotto scalare standard).
	\\
	\QEDB
\end{es}


\noindent
Il fatto che il nullspace sia ortogonale allo spazio delle righe ha ripercussioni utili nel caso di alcune applicazioni lineari. Data una funzione con matrice associata simmetrica (quindi $f$ autoagginto) si ha che il kernel è ortogonale al sottospazio immagine dell funzione.
\[
\text{Poiché f simmetrica si ha che } \quad R(A) = C(A) \quad \text{con } C(A) = \text{ in componenti a } Im(f) 
\]
Quindi
\[
N(A) \perp R(A) \to \text{ker}f = Im(f)^\perp
\]

\subsection{Rank}
Possiamo definire il rango di una matrice in tre modi diversi.
\begin{enumerate}
	\item Numero di righe non nulle della matrice ridotta a scala;
	\item Dimensione dello spazio delle righe o dello spazio delle colonne;
	\item Massimo ordine di un minore non nullo della matrice.
\end{enumerate}





%% %% %%
%% %% %%  APPLICAZIONI LINEARI
%% %% %% 

\newpage
\section{Applicazioni Lineari}
Le applicazioni lineari sono particolari tipi di \textit{funzioni} che preservano la struttura di spazio vettoriale.
\[
f: V \longrightarrow W \qquad V,W \text{ sottospazi vettoriali}
\] 


\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Applicazione lineare}
	Dati due spazi vettoriali reali $V,W$, si dice applicazione lineare o \textbf{omomorfismo} o trasformazione lineare da $V$ in  $W$ una funzione
\[
f: V \longrightarrow W 
.\] 
che verifica le seguenti proprietà:
\[
f\left(\bar{0}_{v}\right) = \bar{0}_{W}
\] 
\[
f\left(x+y\right) = f\left(x\right) + f\left(y\right)
\] 
\[
f\left(\lambda x + \mu y\right) = \lambda f\left(x\right) + \mu f\left(y\right) 
.\] 
per ogni $x$ e  $y$ in $V$ e per ogni $\lambda$ e $\mu$ in $\mathbb{R}$.
\end{blues}
\end{minipage}}        
\end{center}
\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema fondamentale delle applicazioni lineari}
				Dati $V$ e  $W$ spazi vettoriali,
				\[
				\mathscr{B} = \{\bar{v}_{1},\dots,\bar{v}_{n}\} \text{ base di }V
				\] 
				\[
				\mathscr{C} = \{\bar{a}_{1},\dots,\bar{a}_{n}\} \text{ immagini di vettori in }W
				.\] 
				Allora esiste ed è \textbf{unica} l'applicazione lineare
				\[
				f: V \longrightarrow W \text{ t.c. }
				\] 
				\[
				f\left(\bar{v}_{i}\right) = \bar{a}_{i} \quad \forall i \in \{1,\dots,n\}
				.\] 
			\end{redes}
	\end{minipage}}        
\end{center}

In altre parole per assegnare un'applicazione lineare tra due spazi vettoirali $V$ e $W$, di cui almeno $V$ di dimenzione finita, è sufficiente conoscere le immagini dei vettori di una vase di $V$.


\begin{es}{dimostrazione}
\[
\bar{x} \in V
.\] 
\[
\bar{x} = x_1 \bar{v}_{1} + x_2\bar{v}_{2} + \dots + x_{n}\bar{v}_{n}
.\] 
\begin{align*}
	f\left(\bar{x}\right) &= f\left(x_1 \bar{v}_{1} + x_2\bar{v}_{2} + \dots + x_{n}\bar{v}_{n}\right) \\
				   &= x_1 f\left(\bar{v}_{1}\right) + x_2 f\left(\bar{v}_{2}\right) + \dots + x_{n}f\left(\bar{v}_{n}\right) \\
				   &= x_1 \bar{a}_{1} + x_2 \bar{a}_{2} + \dots + x_{n} \bar{a}_{n}
\end{align*}
Vicecersa se definiamo $f$ dicendo che 
\[
f\left(\bar{x}\right) = x_1 \bar{a}_{1} + x_2 \bar{a}_{2} + \dots + x_{n} \bar{a}_{n}
.\] 
Allora
\begin{itemize}
	\item $f$ è lineare: $\quad f\left(\lambda \bar{x} + \mu \bar{y}\right) = \lambda f\left(\bar{x}\right) + \mu f\left(\bar{y}\right)$.
	\item $f\left(\bar{v}_{i}\right) = \bar{a}_{i} \quad \forall i$
\end{itemize} \QEDB
\end{es}


Quindi definire un'applicazione lineare tra due spazi vettoriali equivale a \textit{conoscere le immagini degli elementi di una base del dominio}.

\subsection{Applicazioni lineari in componenti}
Voglio vedere come si comportano le applicazioni lineari in componenti. Prendo una funzione $f: V \to W$ con basi $\mathcal{B}$ e $\mathcal{C}$ di dominio e codominio e di dimensioni $n$ e $r$. 
\[
\mathcal{B} = \left\{\bar{v}_{1}, \dots, \bar{v}_{n}\right\} \qquad \mathcal{C} = \left\{\bar{w}_{1},\dots,\bar{w}_{r}\right\}
\]

\noindent
Partiamo prendendo un vettore nel dominio:
\[
\bar{x} \in V \qquad \left[\bar{x}\right]_{\mathcal{B}} = \left(\begin{array}{c}
	x_{1} \\
	\vdots \\
	x_{n}
\end{array}\right) \qquad \bar{x} = x_{1}\bar{v}_{1} + \dots + x_{n}\bar{v}_{n}
\]
ora applichiamo la trasformazione ad esso:
\begin{align*}
	\left[f(\bar{x})\right]_{\mathcal{C}} =& \left[f(x_{1}\bar{v}_{1} + \dots + x_{n}\bar{v}_{n})\right]_{\mathcal{C}}	\\
							=& \left[x_{1}f(\bar{v}_{1}) + \dots + x_{n}f(\bar{v}_{n})\right]_{\mathcal{C}} \\
							=& x_{1}\left[f(\bar{v}_{1})\right]_{\mathcal{C}} + \dots + x_{n}\left[f(\bar{v}_{n})\right]_{\mathcal{C}}
\end{align*}

\[
= \left[\begin{array}{c|c|c}
	& & \\
	\left[f(\bar{v}_{1})\right]_{\mathcal{C}} & \dots & \left[f(\bar{v}_{n})\right]_{\mathcal{C}}\\
	& &
\end{array}\right]\left[\begin{array}{c}
x_{1} \\ \vdots \\ x_{n}
\end{array}\right]
\]

\[
\left[f(\bar{x})\right]_{\mathcal{C}} = M^\mathcal{B}_{\mathcal{C}}(f)\left[\bar{x}\right]_{\mathcal{B}}
\]


\subsection{Suriettività e Iniettività}
\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
	\subsubsection{Teorema: Suriettivià}
Data l'applicazione lineare $f: V \to W$, $f$ si dice suriettiva se 
\[
\text{im}f = W
\]
o se 
\[
\text{rank}(A) = \text{dim}W
\]
	\end{redes}
	\end{minipage}}        
\end{center}
			

\noindent
Una funzione è suriettiva se il sottospazio immagine corrisponde al codominio, per questo non può esistere un'isola, no dai, non può esistere una funzione suriettiva del tipo $f:\mathbb{R}^2 \to \mathbb{R}^3$ (si ha che $\text{Im}f$ ha dimensione massima 2, quindi non potrà mai generare tutto $\mathbb{R}^3$).


\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
			\subsubsection{Teorema: Iniettività}
			Data l'applicazione lineare $f: V \to W$, $f$ si dice suriettiva \textit{se l'immagine di ogni insieme libero di $V$ è un insieme libero di $W$}. \\
			
			Oppure se 
			\[
			\text{ker}f = \{\bar{0}_{V}\}
			\]
				
				
			\end{redes}
	\end{minipage}}        
\end{center}
				

\noindent
Mandando insiemi liberi in insiemi liberi, l'unico vettore che può essere mappato in $0$ è solo $\{\bar{0}\}$. 

\subsubsection{Isomorfismo e automorfismo}

Con $\text{dim}V = \text{ dim}W \qquad f \text{ iniettiva se e solo se } f \text{ suriettiva}.$

\begin{enumerate}
	\item $f$ è un \textbf{isomorfismo} $\Longleftrightarrow$ $\text{ker}f = \{\bar{0}_{V}\}$
	\item $f$ è un \textbf{isomorfismo} $\Longleftrightarrow$ $\text{Im}f = W$
\end{enumerate}

Se invece prendiamo un endomorfismo $f: V \to V$
\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Automorfismo}
				Un endomorfismo \textbf{anche biettivo} (quindi isomorfismo) lo chiamiamo \textbf{automorfismo}. Dato $f: V \rightarrow V$ posso associare a  $f$ la matrice rappresentativa $M^\mathscr{B}\left(f\right)$.
				\[
				f \text{ automorfismo} \quad \Longleftrightarrow \quad  M^\mathscr{B}\left(f\right) \text{ invertibile} \quad \Longleftrightarrow \quad \text{det}M^{\mathscr{B}}\left(f\right) \neq 0
				\] 
			\end{blues}
	\end{minipage}}        
\end{center}

\begin{enumerate}
	\item $f$ è un \textbf{automorfismo} $\Longleftrightarrow$ $\text{ker}f = \{\bar{0}_{V}\}$
	\item $f$ è un \textbf{automorfismo} $\Longleftrightarrow$ $\text{Im}f = V$
\end{enumerate}

\subsection{Controimmagine}
Teorema fondamentale per comprendere meglio alcuni concetti legati al kernel:
\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: controimmagine}
				Sia $f:V\to W$ e sia $\mathcal{K}$ sottospazio vettoriale di $W$, allora
				\[
				f^{-1}(\mathcal{K}) = \left\{\bar{x} \in V : f(x) \in \mathcal{K}\right\}
				\]
			\end{blues}
	\end{minipage}}        
\end{center}

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				
			\subsubsection{Teorema: $f^{-1}$ è sottospazio}
			La controimmagine $f^{-1}(\mathcal{K})$ di un sottospazio vettoriale $\mathcal{K}$ di $W$ è un sottospazio vettoriale di $V$.	
			\end{redes}
	\end{minipage}}        
\end{center}
				

\begin{es}{dimostrazione}
	Dalla definizione di sottospazio vettoriale bisogna dimostrare che \textit{per ogni} $\bar{x}_{1},\bar{x}_{2} \in f^{-1}(\mathcal{K})$ e per ogni $\lambda, \mu \in \mathbb{R}$ si ha:
	\[
	\lambda \bar{x}_{1} + \mu \bar{x}_{2} \in f^{-1}(\mathcal{K})
	\]
	Quindi dobbiamo dimostrare che 
	\[
	f\left(\lambda\bar{x}_{1} + \mu \bar{x}_{2}\right) \in \mathcal{K}
	\]
	Per linearità segue
	\[
	f\left(\bar{x}_{1} + \mu \bar{x}_{2}\right) = \lambda f\left(\bar{x}_{1}\right) + \mu f\left(\bar{x}_{2}\right)
	\]
	Poiché $\mathcal{K}$ è sottospazio vettoriale l'ultima uguaglianza vale ed è contenuta in esso; allora abbiamo dimostrato la tesi.
	\\
	\QEDB
\end{es}

\subsubsection{Nucleo}
	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Nucleo}
				Chiamiamo nucleo di un'applicazione lineare $f:V\to W$ il sottospazio vettoriale di $V$ controimmagine del sottospazio vettoriale $\{\bar{0}_{W}\}$ e si indica con:
				\[
				\text{ker}f = \left\{\bar{x} \in V | f(x) = \bar{0}_{W}\right\}
				\]
			\end{blues}
	\end{minipage}}       
\end{center}
Il fatto che il kernel sia un sottospazio deriva proprio dal teorema precedente.

%% %% %%
%% %% %%  AUTOVETTORI AUTOVALORI
%% %% %% 
\newpage
\section{Autovettori, autovalori}



\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Sottospazio invariante}
Dato un sottospazio $W$ e un vettore  $w \in W$ il sottospazio si dice invariante se $f\left(w\right) \subseteq W$.

Un sottospazio invariante è, per esempio, un \textit{autospazio}; infatti i vettori di un autospazio sono multipli delle loro immagini: $f\left(\bar{x}\right) = \lambda \bar{x}$.
\end{blues}
\end{minipage}}        
\end{center}

\noindent
Sono sempre sottospazi invarianti:
\begin{enumerate}
	\item Il sottospazio immagine $\text{Im}(f)$;
	\item Il nucleo $\text{ker}(f)$;
	\item Gli autospazi $V_{\lambda_i}$.
\end{enumerate}






\subsection{Autovettori e autovalori}
\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Autovettori}
Data l'endomorfismo $f:V\rightarrow V$ un vettore $\bar{x} \in V \quad \bar{x} \neq \bar{0}$  si dice \textbf{atuovettore} se 
\[
f\left(\bar{x}\right) = \lambda \bar{x}
\].
\begin{itemize}
	\item Lo scalare $\lambda$ è l'\textbf{autovalore} associato a $\bar{x}$.
	\item $V_{\lambda}$ si dice \textbf{autospazio} associato a $\lambda$ ed è l'insieme di tutti gli autovettori $\{\bar{x} \in V: f\left(x\right) = \lambda \bar{x}\}$.
\end{itemize}
\end{blues}
\end{minipage}}        
\end{center}

Quindi chiamiamo autovettori tutti quei vettori che vengono mandati da una certa funzione $f$ in multipli di loro stessi. Ora dobbiamo occuparci di come trovare questi autovettori e i corrispondenti autovalori. 

\noindent
Diciamo di avere $A$ matrice associata all'endomorfismo  $f:V \rightarrow V$, per trovare gli autovalori impostiamo la seguente uguaglianza: 

\begin{align*}
A\bar{x} &= \lambda \bar{x} \\
A\bar{x} - \lambda \bar{x} &= \bar{0}\\
\left(A - \lambda \textcolor{red}{I}\right)\bar{x} &= \bar{0} 
\end{align*}

L'uguaglianza è rappresentata da un sistema omogeneo che ha soluzioni non banali solo se il rango della matrice $A-\lambda I$ non è massimo, quindi solo se il determinante è uguale a $0$.

\[
\text{det}\left(A - \lambda I\right) = 0
\] 

\noindent
Il polinomio risultante dall'equazione viene chiamato \textit{polinomio caratteristico} di $A$:  $P_{A}\left(\lambda\right)$. Le radici del polinomio caratteristico sono i nostri autovalori:

 \[
\text{det}\left(A-\lambda I\right) = \begin{vmatrix}
     a_{11}-\lambda &  &  & \\
     & a_{22}-\lambda &  & \\
     &  & \ddots&  & \\
     & &  & a_{nn}-\lambda 
\end{vmatrix}
= \text{polinomio di grado }n \text{ in } \lambda
\] 

\[
= \left(a_{11} - \lambda\right)\left(a_{22}-\lambda\right)\dots\left(a_{nn}-\lambda\right)
\] 
\[
= \lambda^n + c_{1}\lambda^{n-1} + \dots + c_{n} 
.\] 
\noindent
Dato un certo polinomio caratteristico, è chiamata \textit{molteplicità algebrica} il numero di volte che un certo $\lambda_{0}$ annulla il polinomio: $\text{m}_{\text{a}}\left(\lambda_{0}\right)$.

\paragraph{Un altro punto di vista} Abbiamo detto che per trovare gli autovalori dobbiamo risolvere la sequante equazione:
\[
	\left(A - \lambda I \right)\bar{x} = \bar{0}
\] 
Dal capitolo sul determinante di una matrice sappiamo che il determinante rappresenta un fattore di stretching; nel nostro caso serve che un vettore non nullo ($\bar{x}$) venga mandato in zero dalla matrice $A - \lambda I$, l'unico caso in cui questo è possibile è quando il determinante di tale matrice è uguale a zero.

\paragraph{Autospazi} Vale la pena soffermarsi su cosa sono, come si trovano e su alcune proprietà degli autospazi.
Per trovare un autospazio $V_{\lambda_{0}}$ associato a un autovalore $\lambda_{0}$ si calcola il nullspace della matrice $A-\lambda_{0} I$:
 \[
	 V_{\lambda_{0}} = \text{ (in componenti) } \text{N}\left(A-\lambda_{0} I\right)
\]
\noindent 
Il numero di generatori del nullspace, ovvero la sua dimensione, viene chiamata \textit{molteplicità geometrica}: $\text{m}_{\text{g}}\left(\lambda_{0}\right)$

\begin{enumerate}
		\item Ogni vettore $\bar{x} \in V_{\lambda_{1}},\dots,V_{\lambda_{k}}$ si scrive in modo unico come $\bar{x}_{1} +, \dots,+ \bar{x}_{k}$ con $x_{j} \in V_{\lambda_{j}}$, ovvero gli autospazi associati agli autovalori di un certo polinomio caratteristico sono in \textbf{somma diretta}.

		\noindent
Questo implica 
\[
\text{dim}\left(V_{\lambda_{1}}\oplus,\dots,\oplus V_{\lambda_{k}}\right) = \text{dim}V_{\lambda_{1}} + \dots + \text{dim}V_{\lambda_{k}}
.\] 
\item La molteplicità geometrica di un autospazio è minore o uguale alla molteplicità algebrica:
	\[
	\text{m}_{\text{g}}\left(\lambda_{0}\right) \leq \text{m}_{\text{a}}\left(\lambda_{0}\right)
	.\] 
\end{enumerate}

\begin{es}{dimostrazione}
Diciamo di avere un autospazio $V_{\lambda_{0}}$ di dimensione $k$ ($k = \text{m}_{\text{g}}\left(\lambda_{0}\right)$) sottospazio vettoriale di $V$ di dimensione $n$.

\noindent
Prendo una base di $V_{\lambda_{0}} $$\mathscr{B}' = \{\bar{v}_{1},\dots,\bar{v}_{k}\}$ e la completo ad una base di $V$: 
\[
\mathscr{B} = \{\bar{v}_{1},\dots,\bar{v}_{k},\bar{v}_{k+1},\dots,\bar{v}_{n}\}
\] 
La matrice associata sarà:
\[
M^{\mathscr{B}}\left(f\right) = \begin{bmatrix}
     \lambda_{0}& 0 &  &  &\\
     0&\lambda_{0}  &  &  &\\
     \vdots& 0 &  &  &\\
     0& 0 &  &  &\\
\end{bmatrix}
\] 
Sappiamo che nelle prime $k$ colonne c'è $\lambda_{0}I$, nelle rimanenti $n-k$ colonne invece, non sappiamo dire cosa ci sia. Rimaniamo con una matrice del genere:
 \[
M^{\mathscr{B}}\left(f\right) = \begin{bmatrix}
    \lambda_{o}I_{k} & B \\
     0& C 
\end{bmatrix}
\qquad B \in \mathbb{R}^{k,n-k}, \quad C \in \mathbb{R}^{n-k,n-k}
\] 
Ora tenendo a mente che l'obiettivo è dimostrare che la molteplicità geometrica di $\lambda_{0}$ sia minore o uguale alla molteplicità geometrica, ovvero la molteplicità algebrica (il numero di radici reali del polinomio caratteristico) sia almeno $k$, andiamo a calcolare i nostri lambda.
\[
	 \text{det}\left(M^{\mathscr{B}}\left(f\right) - \lambda I\right) = 
	 \begin{vmatrix}
	\lambda_{o}I_{k} - \lambda I_{k} & B \\
     	0& C - \lambda I 
	\end{vmatrix} = 0
\] 
\[
= \left(\lambda_{0} - \lambda\right)^{k}\left(C-\lambda I\right) = 0
\] 
L'ultima uguaglianza conferma che la molteplcità algebrica di $\lambda_{o}$ sia almeno uguale a $k$.
\\
\QEDB
\end{es}



\paragraph{Teorema}
Una matrice quadrata associata a una funzione $f$ è invertiile se e solo se $\lambda = 0$ non è un suo autovalore. 

\noindent
Se ci riflettiamo, se eiste un $\lambda_0 = 0$, vuol dire che la funzione manda un vettore non nullo in  un vettore nullo:
\[
f\left(\bar{x}\right) = 0\bar{x}
.\] 
Questo ci dice che la funzione è \textit{non-iniettiva} e che di conseguenza non può essere biettiva, condizione necessaria perché una funzione sia invertibile.





\section{Isometrie}
	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Isometria}
				Un'isometria di $V$ (lineare) è un endomorfismo che soddisfa
				\[
				f(\bar{x}) \cdot f(\bar{y}) = \bar{x} \cdot \bar{y} \qquad \forall \bar{x},\bar{y} \in V
				\]
			\end{blues}
	\end{minipage}}       
\end{center}

Nel caso in cui $\bar{x} = \bar{y}$ allora si ha che:
\[
\| f(\bar{x}) \|^2  = \| \bar{x} \|^2 \qquad \text{presereva le lunghezze;}
\]
\[
\frac{f(\bar{x})\cdot f(\bar{y})}{\| f(\bar{x}) \| \cdot \| f(\bar{x}) \|} = \frac{\bar{x}\cdot \bar{y}}{\| \bar{x}) \| \cdot \| \bar{x} \|} \qquad \text{preserva gli angoli}
\] \\

\noindent
Se $f$ endomorfismo preserva le lunghezze, allora \textbf{è forzato a preservare i prodotti scalari}. Inoltre preso $(V,\cdot)$ euclideo e $f\in \text{End}(V)$ sono equivalenti:
\begin{enumerate}
	\item $f$ è un isometria;
	\item $\| f(\bar{x}) \|  = \| \bar{x} \| \quad \forall \bar{x} \in V$;
	\item Se $\left\{\bar{x}_{1},\dots,\bar{x}_{n}\right\}$ sistema ortonormale, allora anche $\left\{f(\bar{x}_{1}),\dots,f(\bar{x}_{n})\right\}$ lo è;
	\item $\forall \mathcal{B}$ base ortonormale, $M^\mathcal{B}(f)$ è ortogonale.
\end{enumerate}

\begin{es}{dimostrazione $(2 \to 1)$}
	\textit{Se preserva le lunghezze $\Longrightarrow$ è un'isometria}.
	Sviluppiamo
	\[
	\| \bar{x} + \bar{y} \|^2 = \|\bar{x}\|^2 + \|\bar{y}\|^2 + 2\bar{x} \cdot \bar{y}
	\]
	e poi
	\[
	\| f(\bar{x}) + f(\bar{y}) \|^2 = \|f(\bar{x})\|^2 + \|f(\bar{y})\|^2 + 2f(\bar{x}) \cdot f(\bar{y})
	\]
	Poiché $f$ preserva le lunghezze si ha che $\| f(\bar{x}) \|  = \| \bar{x} \|$ e quindi anche che $\| \bar{x} + \bar{y} \|^2 = \| f(\bar{x}) + f(\bar{y}) \|^2$ che ci porta a
	\[
	\|\bar{x}\|^2 + \|\bar{y}\|^2 + 2\bar{x} \cdot \bar{y} = \|f(\bar{x})\|^2 + \|f(\bar{y})\|^2 + 2f(\bar{x}) \cdot f(\bar{y})
	\]
	dal fatto che $\| f(\bar{x}) \|  = \| \bar{x} \|$ si annullano i termini al quadrato e dividendo per due da entrambe le parti otteniamo
	\[
	\bar{x} \cdot \bar{y} = f(\bar{x}) \cdot f(\bar{y})
	\]
\end{es} 


\noindent
E' importante ricordare che $M^\mathcal{B}(f)$ è ortogonale solo rispetto a una base ortonormale; bisogna quindi stare attenti al prodotto scalare presente nello spazio, per esempio: \\

\noindent
Prendo in $\mathbb{R}^2$ con base standard $\mathcal{B} = (\bar{e}_{1},\bar{e}_{2})$ e con prodotto scalare
\[
\left(\begin{array}{c}
	x_{1} \\ x_{2}
\end{array}\right) \cdot \left(\begin{array}{c}
y_{1} \\ y_{2}
\end{array}\right) = x_{1}y_{1} + 4x_{2}y_{2}
.\]
Sia poi la funzione
\[
f:\left(\begin{array}{c}
	x_{1} \\ x_{2}
\end{array}\right) \to A\left(\begin{array}{c}
x_{1} \\ x_{2}
\end{array}\right) \qquad A = \left(\begin{array}{cc}
\frac{\sqrt{3}}{2} & 1 \\
-\frac{1}{4} & \frac{\sqrt{3}}{2}
\end{array}\right)
\]
risulta che $f$ \textbf{è un'isometria} ma $M^\mathcal{B}(f)$ \textbf{non è ortogonale}! Questo perché $\mathcal{B}$ non è ortonormale rispetto al prodotto scalare scelto.

\subsubsection{Proprietà delle isometrie}
\begin{enumerate}
	\item Un'isometria è un \textbf{automorfismo} (endomorfismo biettivo);
	\item Se $f,g$ isometrie $\Longrightarrow$ $f \circ g$ isometria:
	\[
	f \circ g  = f(g(\bar{x})) \quad \to \quad \|f(g(\bar{x}))\| = \|g(\bar{x})\| = \|\bar{x}\| 
	\]
	\item L'insieme delle isometrie è un \textbf{gruppo}: $\left\{f \text{ isometrie}\right\} = \text{Iso}(V)$;
	\item Gli autovalori di $f$ isometria possono essere solo $\pm 1$.
\end{enumerate}

\subsubsection{Basi destrorse}
Diciamo che la base di $V_{2}$ $\mathcal{B} = \left\{\bar{v}_{1},\bar{v_{2}}\right\}$ è destrorsa se rispetta la regola della mano destra.



\subsection{Isometrie in $V_{2}$}
Un automorfismo \textbf{preserva l'orientazione} se manda basi destrorse in basi destrorse; studiamo due tipi di isometrie in $V_{2}$, che hanno, rispettivamente, matrici associate
\[
\begin{bmatrix}
	\cos{\theta} & -\sin{\theta} \\
	\sin{\theta} & \cos{\theta}
\end{bmatrix}
\qquad \qquad \qquad \qquad \qquad \qquad
\begin{bmatrix}
	\cos{\theta} & \sin{\theta} \\
	\sin{\theta} & -\cos{\theta}
\end{bmatrix}
\] 

\vspace{1cm}

\begin{center}
	\includesvg[scale=0.35]{figures/isometrie}
\end{center}


\subsection{Isometrie in $V_{3}$}
\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema di Eulero}
				Ogni isometria che preserva l'orientazione è riconducibile a una rotazione attorno ad un asse.
			\end{redes}
	\end{minipage}}        
\end{center}

Il Teorema di Eulero ci porta a dire che esista sicuramente un vettore che funga da asse di rotazione e, di conseguenza, che il suo \textit{autovalore associato} valga $\lambda = 1$.

\begin{es}{dimostrazione}
	Presa l'isometria $f$, se sicuramente diagonalizzabile si hanno tre autovalori reali e si ha il polinomio caratteristico:
	\[
	p(\lambda) = (\lambda_1 - \lambda)(\lambda_2 - \lambda)(\lambda_3 -\lambda) = \dots
	\]
	Sappiamo che il termine noto $\lambda_1 \lambda_2 \lambda_{3} = \text{det}M^\mathcal{B}f = 1$ e che quindi uno dei tre deve per forza valere $1$. 
	
	Se invece abbiamo due autovalori complessi e uno reale ($\lambda_{1}$ reale, $\lambda_{2},\lambda_3$ complessi):
	\[
	1 = \lambda_{1}(\lambda_2 \overline{\lambda}_{2}) = \lambda_1 |\lambda_2|^2 \Longrightarrow \lambda_1 \text{ positivo } \Longrightarrow \lambda_{1} = 1
	\]
	
\end{es}

%% %% %%
%% %% %%  DIAGONALIZZAZIONE
%% %% %% 
\newpage
\section{Diagonalizzazione}
Scopo principale della diagonalizzazione sarà quello di trovare basi di soli autovettori. Il processo è schematizzato in questo modo:
\begin{center}
\includesvg[scale=0.8]{figures/quad}
\end{center}

\noindent
Data una matrice scritta rispetto alla base $B'$ la si riscrive rispetto alla base  $B$, si applica la trasformazione lineare  $f$ e si ritorna alla base  $B'$. 
 \[
 M^{B'}_{B'}\left(f\right) = M^{B}_{B'}\left(\text{id}\right) M^{B}_{B}\left(f\right) M^{B'}_{B}\left(\text{id}\right)
.\] 

\noindent
Quindi se chiamiamo
\[
A = M^{B}_{B}\left(f\right) \qquad A' = M^{B'}_{B'}\left(f\right) \qquad P = M^{B'}_{B}\left(\text{id}\right)
.\] 

\noindent
possiamo riscrivere la precedente come:
\[
A' = P^{-1} A P
.\] 

\noindent
Diremo che $A'$ è  \textbf{simile} a $A$. Le matrici simili condividono molte caratteristiche:
\begin{enumerate}
	\item $A$ e  $P^{-1}AP$ hanno stesso determinante;
\begin{align*}
	\text{det}\left(A'\right) &= \text{det}\left(P^{-1}AP\right)\\ 
		   &= \text{det}\left(P^{-1}\right)\text{det}\left(A\right)\text{det}\left(P\right) \\
		   &= \frac{1}{\text{det}\left(P\right)} \text{det}\left(A\right) \text{det}\left(P\right) \\
		   &=\text{det}\left(A\right)
\end{align*}
\item $A$ invertibile  $\Longleftrightarrow$  $A'$ invertibile;
\item  $A$ e  $A'$ hanno stesso  \textit{rango}, \textit{nullspace}, \textit{polinomio caratteristico}.
\end{enumerate}

\noindent
Altre caratteristiche delle matrici simili sono meno evidenti; per comprenderle a pieno è bene ricordarsi che queste sono rappresentazioni della \textbf{stessa trasformazione lineare} solo rispetto una base differente. Questo vuole dire che matrici simili \textbf{preservano anche la struttura degli autospazi} che avranno quindi stesse dimensioni, proprio perché le due matrici rappresentano la stessa applicazione lineare. Se chiamiamo $\bar{v}$ gli autospazi rispetto alla matrice associata $A$ e $\bar{v}'$ gli autospazi rispetto alla matrice associata $P^{-1}AP = A'$, allora si ha che 
\[
\bar{v}' = P\bar{v}
\]

\noindent
Allo scopo di diagonalizzare una matrice le matrici simili sono essenziali, infatti, una matrice quadrata $A$ si dice \textbf{diagonalizzabile} se è simile a un'altra matrice diagonale.

\subsection{Criteri di diagonalizzabilità}
 \[
f: V \longrightarrow V \text{ endomorfismo.} \quad \text{ Sono equivalenti: }
\] 
\begin{enumerate}
	\item $f$ diagonalizzabile ;
	\item $P_{f}\left(\lambda\right)$ ha solo radici reali e per ogni $\lambda_{j}$ si ha $\text{m}_{\text{g}}\left(\lambda_{j}\right) = \text{m}_{\text{a}}\left(\lambda_{j}\right)$ ;
	\item Se gli autovalori sono tutti distinti non nulli, $\text{dim}\left(V_{\lambda_{1}}\right) + \dots + \text{dim}\left(V_{\lambda_{k}}\right) = \text{dim}\left(V\right)$;
	\item Se gli autovalori sono tutti distinti, $V = V_{\lambda_{1}} \oplus \dots \oplus V_{\lambda_{k}}$.
\end{enumerate}

\begin{es}{dimostrazione $1 \Rightarrow 2$}
Sia  $f$ diagonalizzabile e  $\mathscr{B}$ base di autovettori.
\begin{center}
\includesvg[scale=0.7]{figures/matriceblu}
\end{center}

\[
P_{f}\left(\lambda\right) = \text{det}\left(M^{B}\left(f\right)-\lambda I\right) \qquad = \left(\lambda_1 - \lambda\right)^{m_1}\left(\lambda_2 - \lambda\right)^{m_2} \dots \left(\lambda_{k} - \lambda\right)^{m_{k}}
\] 
Il polinomio ha solo radici reali; il massimo grado del polinomio lo si trova sommando gli esponenti:
\[
m_1 + m_2 + \dots + m_{k} = \text{dim}V
\]
Infatti il numero di colonne della matrice è uguale alla dimensione di $V$ (essendo le colonne sicuramente linearmente indipendenti) ed è uguale al numero di righe, uguale a $m_1 + \dots + m_{k}$. \\ 

Ora dobbiamo dimostrare che per ogni $\lambda_{j}$ si ha $\text{m}_{\text{g}}\left(\lambda_{j}\right) = \text{m}_{\text{a}}\left(\lambda_{j}\right)$.

Troviamo il sottospazio associato a $\lambda_1$:

\[
V_{\lambda_{1}} = \text{N}\left(M^B\left(f\right) - \lambda_1 I\right)
\] 

\begin{center}
\includesvg[scale=0.7]{figures/matricelunga}
\end{center}

Le prime righe (contrassegnate da $m_1$ ) sono nulle, quindi il rank della matrice è $\text{rk}M^B\left(f\right) = \text{dim}V - m_1$.  Quindi la dimensione del nullspace è
\[
\text{dim}V - \text{rank}\left(A-\lambda_1 I\right) = m_1
\] 

\[
\Longrightarrow \text{m}_{\text{g}}\left(\lambda_{1}\right) = \text{m}_{\text{a}}\left(\lambda_{1}\right) = m_1
\] 
\\
\QEDB
\end{es}

\begin{es}{dimostrazione $2 \Rightarrow 3$}
Sappiamo che il polinomio caratteristico si scompone in equazioni lineari:
\[
P_{f}\left(\lambda\right) = \left(\lambda_1 - \lambda\right)^{m_1}\dots\left(\lambda_{k} - \lambda\right)^{m_{k}}
.\] 
Dalla dimostrazione precedente sappiamo che la molteplicità algebrica è uguale a quella geometrica. Ricordando che $\text{m}_{\text{g}}\left(\lambda_{j}\right)=\text{dim}\left(V_{\lambda_{j}}\right)$ e che $m_j = \text{m}_{\text{a}}\left(\lambda_{j}\right)$

\[
m_1 + m_2 + \dots + m_{k} = \text{dim}V
\] 
\[
\text{m}_{\text{g}}\left(\lambda_{j}\right) = \text{m}_{\text{a}}\left(\lambda_{j}\right)
\] 
\[
 \text{m}_{\text{a}}\left(\lambda_{1}\right) + \dots +  \text{m}_{\text{a}}\left(\lambda_{k}\right) = \text{dim}V
\]
\[
\Longrightarrow \text{dim}\left(V_{\lambda_{1}}\right) + \dots + \text{dim}\left(V_{\lambda_{k}}\right) = \text{dim}V
.\]  \QEDB
\end{es}

\begin{es}{dimostrazione: $3 \Rightarrow 4$}
\[
\text{dim}\left(V_{\lambda_{1}} \oplus \dots \oplus V_{\lambda_{k}}\right) = \text{dim}\left(V_{\lambda_{1}}\right) + \dots + \text{dim}V_{\lambda_{k}} = \text{dim}V
\] 
Sappiamo infatti che $V_{\lambda_{1}} \oplus \dots \oplus V_{\lambda_{k}}$ è un sottospazio di $V$ con la stessa dimensione.
\\
\QEDB
\end{es}

\begin{es}{dimostrazione $4 \Rightarrow 1$}
So che $V$ si decompone in somma diretta di autospazi
\[
V = V_{\lambda_{1}} \oplus \dots \oplus V_{\lambda_{k}}
\] 
Ora prendo una base per ogni autospazio
\[
\begin{array}{ccc}
	B_1 & \text{base per } & V_{\lambda_{1}} \\
	\vdots & & \vdots \\
	B_k & \text{base per } & V_{\lambda_{k}} \\
\end{array}
\] 
Sappiamo che $B$ base di V si ottiene unendo tutte le basi $B_1, \dots, B_{k}$ degli autospazi:
\[
B = B_1 \cup \dots \cup B_{k}
\]
$B$ è fatta da autovettori, quindi $f$ diagonalizzabile.
\\
\QEDB
\end{es}



\subsection{Endomorfismi autoaggiunti}

\begin{center}
\fboxsep11pt
\colorbox{myblue}{\begin{minipage}{5.75in}
\begin{blues}{Definizione: Endomorfismo autoaggiunto}
Chiamiamo \textbf{autoaggiunto} l'endomorfismo  nel qual vale:\[
f\left(\bar{x}\right)\cdot \bar{y} = \bar{x}f\left(\bar{y}\right) \qquad \forall\bar{x},\bar{y} \in V
\] 
\[
a_{ij} = a_{ji}
\]
\end{blues}
\end{minipage}}        
\end{center}

\noindent
Presa una funzione $f \in \text{End}(V)$ con $\left(V,\cdot\right)$ Euclideo, sono equivalenti
\begin{enumerate}
	\item $f$ autoaggiunta;
	\item $\forall \mathcal{B}$ ortonormale $M^\mathcal{B}(f)$ è \textbf{simmetrica};
	\item $\exists \mathcal{B}$ ortonormale t.c. $M^\mathcal{B}(f)$ è \textbf{simmetrica}.
\end{enumerate}

\begin{es}{dimostrazione $1 \Longrightarrow 2$}
	In $(V,\cdot)$ euclideo prendiamo una base ortonormale $\mathcal{B} = \left\{\bar{v}_{1},\bar{v}_{2},\dots,\bar{v}_{n}\right\}$. Notiamo che il pro dotto scalare tra un qualsiasi vettore di $V$ con un vettore j-esimo della base restituirà la componente j-esima del vettore:
	\[
	\bar{u} \cdot \bar{v}_{j} = u_{j}
	\]
	Allora si ha che
	\[
	\text{i-esima componente di } f(\bar{v}_{j}) \qquad f(\bar{v}_{j}) \cdot \bar{v}_{i} = \bar{v}_{j} \cdot f(\bar{v}_{i}) \qquad \text{j-esima componente di } f(\bar{v}_{i})
	\] \QEDB
\end{es}

\noindent
E' importante notare che gli autospazi associati a un'applicazione lineare autoaggiunta sono \textbf{ortogonali} tra loro.

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				
			\subsubsection{Teorema spettrale}
			Dato $\left(V,\cdot\right)$ euclideo e $f \in \text{End}\left(V\right)$, allora: 
			\[
			f \text{ autoaggiunto} \Longleftrightarrow \exists \mathcal{B} \text{ ortonormale di autovettori}
			\]
			In particolare:
			\[
			f \text{ autoaggiunto}  \quad \Rightarrow \quad f \text{ diagonalizzabile}
			.\] 	
			\end{redes}
	\end{minipage}}        
\end{center}



\begin{es}{dimostrazione}
$\Longleftarrow \quad$ Diretta conseguenza della proposizione: se $B$ ortonormale di autovettori, $M^B\left(f\right)$ è diagonale (quindi simmetrica). 

$\Longrightarrow \quad$ Dimostriamo che esiste una base ortonormale di autovettori in tre punti:
\begin{enumerate}
	\item Voglio solo radici reali;
	\item Mostro che gli autospazi sono ortogonali fra loro;
	\item Mostro che $V_{\lambda_{1}} \oplus^\perp \dots \oplus^\perp V_{\lambda_{k}} = V$.
\end{enumerate} 

$1) \quad$ \textit{$f$ autoaggiunto $\Longrightarrow$ $P_{f}(\lambda)$ ha solo radici reali.} 

Il polinomio $P_{f}(\lambda)$ avrà $n$ radici complesse ($\lambda \in \mathbb{C}$). Sappiamo che ogni radice complessa viene accoppiata con il suo coniugato, per questo, faremo vedere che vale
\[
\lambda = \bar{\lambda} 
\]
Troviamo gli autovalori:
\[
\text{det}\left(A - \lambda I\right) = 0 
\]
\[
\exists X \in \mathbb{C}^n \quad \text{t.c.} \quad AX = \lambda X
\]
Trovo lambda coniugato:
\[
\overline{AX} = \overline{\lambda X} \to A\overline{X} = \overline{\lambda} \overline{X}
\]

Ho trovato che $\overline{X}$ è autovettore di $A$ con autovalore $\overline{\lambda}$. Adesso mostriamo che $\lambda = \bar{\lambda}$:
\[
\leftindex^{t}{\bar{X}}AX = \leftindex^{t}{\bar{X}} \lambda X = \lambda \leftindex^{t}{\bar{X}}  X
\]
\[
(\leftindex^{t}{\bar{X}}A)X = \leftindex^{t}{(A\bar{X})}X = \bar{\lambda} \leftindex^{t}{\bar{X}} X
\]

poiché siamo partiti da due espressioni uguali la loro differenza deve essere uguale a zero:
\[
\lambda \leftindex^{t}{\bar{X}}  X -  \bar{\lambda} \leftindex^{t}{\bar{X}} X = 0 \to (\lambda - \bar{\lambda})(\leftindex^{t}{\bar{X}} X) = 0
\]
\end{es}
\begin{es}{}
poiché
\[
\leftindex^{t}{\bar{X}} X = (\bar{x}_{1} \dots \bar{x}_{n}) \cdot \left(\begin{array}{c}
	x_{1} \\ \vdots \\ x_{n}
\end{array}\right)
= |x_{1}|^2 + \dots + |x_{n}|^2 > 0
\]
sappiamo che 
\[
(\lambda - \bar{\lambda})(\leftindex^{t}{\bar{X}} X) = 0 \Longleftrightarrow \lambda = \bar{\lambda}
\]

$2) \quad$ \textit{Gli autospazi sono ortogonali fra loro}.

Prendo due vettori appartenenti a due autospazi diversi:
\[
\bar{x} \in V_{\lambda}, \bar{y} \in V_{\mu} \qquad \lambda \neq \mu
\]
\begin{align*}
	f(\bar{x})& \cdot \bar{y} = \bar{x}\cdot f(\bar{y}) \\
	\lambda \bar{x}& \cdot \bar{y} = \bar{x} \cdot \mu \bar{y} \\
\end{align*}
\[
(\lambda - \mu)\bar{x} \cdot \bar{y} = 0 \Longleftrightarrow \bar{x} \cdot \bar{y} = 0
\]
\\

$3) \quad$ \textit{Gli autospazi sono in somma diretta-ortogonale}.

Per dimostrare che la somma degli autospazi coincide con tutto $V$ è necessario mostrare $W$ insieme degli autospazi è $ = V$, e quindi che $W^\perp = \{\bar{0}\}$. Procederemo in questo ordine:
\begin{enumerate}
	\item Iniziamo col mostrare che $W^\perp$ è un sottospazio invariante;
	\item Studiando la restrizione di $f$ a $W^\perp$ mostriamo che in esso non possono esserci autovettori.
\end{enumerate}

$3.1) \quad$ Se $W \leq V$ è invariante per $f$, allora lo è anche $W^\perp$. Voglio far vedere che se $\bar{x} \in W^\perp$ allora $f(\bar{x})  \in W^\perp$. Diciamo di avere un vettore $\bar{w} \in W$:

\[
f(\bar{x}) \cdot \bar{w} = \bar{x} \cdot f(\bar{w})  = 0
\]
quindi $f(\bar{x}) \in W^\perp$ e $W^\perp$ è sottospazio invariante. \\ \\

$3.2) \quad$ Ora supponiamo che $W \neq V$ e che di conseguenza $W^\perp \neq \{\bar{0}\}$. Studiamo la restrizione di $f$ a $W^\perp$:
\[
f: W^\perp \to W^\perp 
\]
Visto che vale
\[
f(\bar{x}) \cdot \bar{y} =  \bar{x}  \cdot f(\bar{y})  \qquad \forall \bar{x},\bar{y} \in V
\]
$\forall\bar{x},\bar{y} \in W^\perp \quad$ $f|_{W^\perp}$ è \textbf{autoaggiunto} e possiamo usare le dimostrazioni precedenti. Per lo \textit{step 1} infatti $P_{f|_{W^\perp}}(\lambda)$ ha solo radici reali e quindi
\[
\exists \bar{x} \in W^\perp, \exists \lambda \in \mathbb{R} \quad \text{t.c.} \quad f(\bar{x}) = \lambda \bar{x}
\]
Questo ci dice che esistono autovettori $\bar{x} \in W^\perp$ e che quindi $W^\perp$ deve essere contenuto in $W$ insieme che contiene tutti gli autospazi, vero solo se $W^\perp = {\bar{0}}$ e quindi $W = V$.
\\
\QEDB
\end{es}



\paragraph{Corollario}
Sia una matrice $A$ simmetrica, $A$ è sempre diagonalizzabile. Esiste allora una matrice  $D$ diagonale e una $P$ invertibile tale che:
 \[
D = P^{-1}AP
\] 
E' inoltre possibile individuare una matrice ortogonale $Q$ tale che:
\[
D = Q^{-1}AQ = \leftindex^t{Q}AQ
.\] 


Dato l'endomorfismo $f: V \longrightarrow V$ con  $\left(V,\cdot\right)$ euclideo, sono equivalenti:

\begin{enumerate}
	\item $f$ autoaggiunto;
	\item $\forall B$ ortonormale, $M^B\left(f\right)$ è simmetrica.
	\item $\exists B$ ortonormale t.c. $M^B\left(f\right)$ simmetrica.
\end{enumerate}

\begin{es}{dimostrazione $1\Rightarrow 2$}
	Prendo $B = \{\bar{v}_{1},\dots,\bar{v}_{n}\}$ base ortonormale e due vettori $\bar{x}$ e $\bar{y} \in V$.\\
	
	Scritti come vettori colonna abbiamo $X$,$Y$,$AX$ e  $AY$ rispettivamente per  $x$,  $y$,  $f\left(x\right)$ e $f\left(y\right)$. 
	
	Possiamo riscrivere (poiché il prodotto è standard)
	\[
	f\left(\bar{x}\right)\cdot \bar{y} = \bar{x}f\left(\bar{y}\right)
	\] 
	come 
	\[
	\leftindex^t{\left(AX\right)}Y = \leftindex^t{X}AY
	\] 
	\[
	\leftindex^t{X}\leftindex^t{A}Y = \leftindex^t{X}AY
	.\] 
	dalla quale risulta che $A = \leftindex^t{A}$.
	\\
	\QEDB
\end{es}


\newpage
\section{Forme bilineari}
	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Forma bilineare}
			Una forma bilineare su uno spazio vettoriale $V$ è una funzione
			\[
			\phi: V\times V \to \mathbb{R}
			\]
			per cui valgono le seguenti proprietà:
			\begin{enumerate}
				\item $\phi (\bar{x} + \bar{x}',\bar{y}) = \phi (\bar{x},\bar{y}) + \phi (\bar{x}',\bar{y})$
				\item $\phi (\bar{x}, \bar{y}' + \bar{y}') = \phi (\bar{x},\bar{y}) + \phi (\bar{x},\bar{y}')$
				\item  $\phi(\lambda\bar{x},\bar{y}) = \lambda \phi(\bar{x},\bar{y})$
			\end{enumerate}
			\end{blues}
	\end{minipage}}       
\end{center}

\noindent
Studieremo principalmente le forme bilineari simmetriche, ovvero quelle che soddisfano
\[
\phi(\bar{x},\bar{y}) = \phi (\bar{y},\bar{x})
\]
Ogni prodotto scalare è un esempio di FBS e poiché ci si propone di studiare opportune generalizzazione di prodotto scalare, prenderemo in considerazione solo FBS.

Da notare come sia possibile definire in modo naturale na struttura di spazio vettoriale su $\mathbb{R}$ sull'insieme delle forme bilineari simmetriche; possiamo infatti definire l'operazione di somma:
\[
\phi _{1} + \phi_2(\bar{x},\bar{y}) = \phi_{1}(\bar{x},\bar{y}) + \phi_{2}(\bar{x},\bar{y}) \qquad \bar{x},\bar{y} \in V
\]
e l'operazione di prodotto per uno scalare:
\[
(\lambda\phi)(\bar{x},\bar{y}) = \lambda\phi(\bar{x},\bar{y})\qquad \bar{x},\bar{y} \in V, \lambda \in \mathbb{R}
\]
\subsection{Matrice associata alla FBS}
Definiamo una base di V $\mathcal{B} = \left(\bar{v}_{1},\dots,\bar{v}_{n}\right)$ e due vettori $\bar{x},\bar{y}\in V$, è possibile esprimere $\phi (\bar{x},\bar{y})$ in termini dei vettori della base:
\begin{align*}
	\phi(\bar{x},\bar{y}) = \sum_{i,j=1}^{n}x_{i}y_{j}\phi(\bar{v}_{i},\bar{v}_{j})
\end{align*}
La matrice associata sarà quindi
\[
M^{\mathcal{B}}\phi = 
\begin{bmatrix}
	\phi(\bar{v}_{1},\bar{v}_{1}) & \phi(\bar{v}_{1},\bar{v}_{2}) & \dots & \phi(\bar{v}_{1},\bar{v}_{n})  \\
	\phi(\bar{v}_{2},\bar{v}_{1}) & \phi(\bar{v}_{2},\bar{v}_{2}) & \dots & \phi(\bar{v}_{2},\bar{v}_{n})  \\
	\vdots & \vdots & \vdots & \vdots \\
	\vdots & \vdots & \vdots & \vdots \\
	\phi(\bar{v}_{n},\bar{v}_{1}) & \phi(\bar{v}_{n},\bar{v}_{2}) & \dots & \phi(\bar{v}_{n},\bar{v}_{n})  \\
\end{bmatrix} = A
\]
e vale
\[
\phi(\bar{x},\bar{y}) = \leftindex^{t}{X}AY
\]

\subsection{Forma quadratica}
Una volta definito un nuovo tipo di prodotto scalare serve un nuovo concetto di \textit{norma} di un vettore $\|\bar{x}\| = \bar{x} \cdot \bar{x}$.
	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Forma quadratica}
			Data la forma bilineare $\phi \in B_{s}(V,\mathbb{R})$, l'applicazione:
			\[
			Q:V\to \mathbb{R}, \qquad \bar{x} \mapsto Q(\bar{x}) = \phi(\bar{x},\bar{x})
			\]
			si dice forma quadrata associata alla forma bilineare associata.
			\end{blues}
	\end{minipage}}       
\end{center}

E' facile notare che le forme quadratiche non sono applicazioni lineari:
\[
Q(\lambda\bar{x}) = \phi(\lambda\bar{x},\lambda\bar{x}) = \lambda^2Q(\bar{x})
\]
\[
Q(\bar{x} + \bar{y}) = Q(\bar{x}) + 2\phi(\bar{x},\bar{y}) + Q(\bar{y})
\]
\subsection{$\phi$-ortogonalità}
Due vettori $\bar{x},\bar{y}$ sono $\phi$-ortogonali se $\phi (\bar{x},\bar{y}) = 0$. Definiamo quindi il complemento ortogonale di un certo sottospazio $W \leq V$ come:
\[
W^{\phi \perp} = \left\{\bar{x} \in V \quad \text{t.c.} \quad \phi(\bar{x},\bar{y}) = 0 \quad \forall \bar{y} \in W \right\}
\]

	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Nucleo di una forma bilineare}
			Il nucleo di una forma bilineare è il sottospazio contenente tutti i vettori \textit{ortogonali a tutto lo spazio} rispetto a $\phi$, ovvero che annullano la forma bilineare:
			\[
			\phi(\bar{x},\bar{y}) = 0 \quad \forall \bar{y} \in V
			\]
			Quindi
			\[
			\text{ker}\phi = \left\{\bar{x} \in V | \phi(\bar{x},\bar{y}) = 0 \quad \forall \bar{y} \in V\right\}
			\]
			\end{blues}
	\end{minipage}}       
\end{center}

\noindent
In componenti il nucleo è 
\[
\phi(\bar{x},\bar{y}) = 0 \quad  = \leftindex^{t}{X}AY = 0 \qquad \forall y \in V
\]
visto che deve valere per ogni $\bar{y}$ posso eliminarlo dall'equazione; inoltre so che $\leftindex^{t}{A}X = AX$ perché A è simmetrica. Quindi posso riscrivere l'equazione come
\[
 AX = 0
\]
accorgendomi che il kernel di $\phi$ corrisponde (in componenti) con il \textit{nullspace} di A.
	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: Cono isotropo}
				Chiamiamo cono isotropo l'insieme dei vettori \textit{ortogonali a loro stessi} rispetto a $\phi$, ovvero che annullano la forma quadratica:
				\[
				\phi(\bar{x},\bar{x}) = Q(x) = 0
				\]
				Quindi
				\[
				\mathcal{I}_\phi = \left\{\bar{x} \in V | Q(\bar{x}) = 0 \right\}
				\]
			\end{blues}
	\end{minipage}}       
\end{center}
Notiamo che se un vettore è $\phi$-ortogonale a tutto lo spazio allora è anche $\phi$-ortogonale anche a se stesso; quindi il kernel di una certa forma bilineare simmetrica è sempre contenuto nel cono isotropo.
\[
\text{ker}\phi \subseteq \mathcal{I}_{\phi}
\]

\paragraph{Alcuni fatti sulla $\phi$-ortogonalità} A differenza del prodotto scalare, dato un sottospazio $W \leq V$, non vale sempre
\[
\left(W^{\phi \perp}\right)^{\phi \perp} = W
.\]
Se il sottospazio ha $\text{ker}(\phi)$ non banale allora esistono uno o più vettori $\phi$-ortogonali a tutto lo spazio che fanno sì che al doppio ortogonale vengano "aggiunti" dei generatori. Vale il contrario se e solo se $\phi$ è non-degenere. \\

\noindent
Se $W$ non contiene vettori isotropi vale 
\[
W \oplus W^{\phi \perp} = V
\]
Se $W$ contiene $\bar{x}$ isotropo, allora questo, essendo $\phi$-ortogonale a se stesso, sarà contenuto anche in $W^{\phi \perp}$. Quindi $W$ e $W^{\phi \perp}$ hanno intersezione non nulla e non possono essere in somma diretta.


\subsubsection{Classificazione forma quadratica}
\begin{enumerate}
	\item \textbf{Semidefinita} positiva (negativa) se $Q(\bar{x}) \geq 0 (\leq 0)$ ma esistono vettori che annullano la forma quadratica.
	\item \textbf{Definita} positiva (negativa) se $Q(\bar{x}) \geq 0 (\leq 0), \quad Q(\bar{x}) = 0 \Longleftrightarrow \bar{x} = 0$
	\item \textbf{Indefinita} se $Q(\bar{x})$ ha segno variabile al variare dei vettori di V.
\end{enumerate}

\noindent
Diciamo che una forma bilineare è \textit{degenere} se il suo nucleo non è nullo.

Una fbs definita positiva non può mai essere degenere, infatti:
\[
Q(\bar{x}) = 0 \Longleftrightarrow \bar{x} = 0
\]
quindi ha nucleo nullo.

\subsection{Cauchy-Schwarz e Minkowski}
Ora dobbiamo introdurre due disuguaglianze fondamentali per comprendere alcuni concetti chiave:

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Disuguaglianza di Cauchy-Schwarz}
				Se una forma bilineare simmetrica è definita positiva, allora vale
				\[
				| \phi(\bar{x},\bar{y}) | \leq \sqrt{\phi(\bar{x},\bar{x})} \sqrt{\phi(\bar{y},\bar{y})}
				\] 
				e quindi
				\[
				\left[\phi(\bar{x},\bar{y})\right]^2\leq Q(\bar{x})Q(\bar{y}), \quad \forall \bar{x},\bar{y} \in V
				\]
			\end{redes}
	\end{minipage}}        
\end{center}
\begin{es}{dimostrazione}
	\[
	Q(\lambda \bar{x}+\bar{y}) = \lambda^2Q(\bar{x}) + 2\lambda\phi(\bar{x},\bar{y}) + Q(\bar{y})
	\]
	Essendo $\phi(\bar{x},\bar{y})$ definita positiva si ha che $Q(\bar{x}) \geq 0$. Quindi il discriminante ridotto $\left(\frac{b}{2}\right)^2 -ac$ del trinomio deve essere negativo (se lo immagina come una parabola sempre con ordinata non negativa con concavità verso l'alto, questa non deve può avere più i un intersezione con l'asse x):
	\[
	\left[\phi(\bar{x},\bar{y})\right]^2\leq Q(\bar{x})Q(\bar{y})
	\] \QEDB
\end{es}
Si noti che se $\bar{x}$ è un vettore isotropo, quindi vale $Q(\bar{x}) = 0$, allora
\[
\left[\phi(\bar{x},\bar{y})\right]^2\leq 0\cdot Q(\bar{y}) \quad \to \quad \phi(\bar{x},\bar{y}) = 0
\]
che mostra che rispetto a forme bilineari simmetriche semidefinite positive \textit{se un vettore è isotropo, allora è anche nel nucleo}. Questo è esteso alle definite positive per le quali vale $Q(\bar{x}) = \phi(\bar{x},\bar{y}) = 0$, il nucleo e il cono isotropo coincidono nel vettore nullo. \\

\noindent
Poiché il kernel è un sottospazio vettoriale, se il cono isotropo coincide con esso anch'esso lo è. L'unico caso in cui il cono isotropo non è uno spazio vettoriale e se riferito a una forma bilineare simmetrica \textit{indefinita}.\\

\noindent
Diciamo di avere due vettori $\bar{x},\bar{y}$ isotropi:
\[
Q(\bar{x}) = 0, \quad Q(\bar{y}) = 0
\]
\[
Q(\bar{x} + \bar{y}) = Q(\bar{x}) + 2 \phi(\bar{x},\bar{y}) + Q(\bar{y}) = 0 + 2 \phi(\bar{x},\bar{y}) + 0
\]
non so se il termine in $\phi$ vale zero, quindi il cono isotropo non è chiuso per la somma; lo sarebbe se $\bar{x}$ o $\bar{y}$ appartenessero al nucleo di $\phi$.

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Disuguaglianza di Minkowsky}
				\[
				\sqrt{Q(\bar{x}+\bar{y})} \leq \sqrt{Q(\bar{x})} + \sqrt{Q(\bar{y})}
				\]
			\end{redes}
	\end{minipage}}        
\end{center}
 
 \begin{center}
 	\fboxsep11pt
 	\colorbox{myred}{\begin{minipage}{5.75in}
 			\begin{redes}{}
 				\subsection{Teorema di esistenza di una forma canonica}
 				Ogni forma bilineare simmetrica ammette una base che la mette in forma canonica.
 			\end{redes}
 	\end{minipage}}        
 \end{center}
\begin{es}{dimostrazione 2}
	Poiché so che la matrice $A$ associata alla forma bilineare simmetrica è anch'essa simmetrica dal \textbf{corollario del teorema spettrale} posso trovare una matrice $P = M^{\mathcal{B'}}_{\mathcal{B}}$ ortogonale tale che 
	\[
	\leftindex^{t}{P}AP = A' 
	\]
	con $A'$ diagonale. Trovata tale $P$ possiamo dire che 
	\[
	M^{\mathcal{B'}}\phi = A' \quad \text{diagonale}
	\] \QEDB
\end{es}

\begin{es}{dimostrazione 2}
	Data la forma bilineare simmetrica $\phi : V \times V \to \mathbb{R}$, l'obiettivo è quello di definire un buon prodotto scalare in modo che  l'endomorfismo definito da
	\[
	f(\bar{x}) \cdot \bar{y} = \phi(\bar{x},\bar{y})
	\]
	sia \textbf{autoaggiunto}. Se l'endomorfismo è autoaggiunto posso utilizzare il teorema spettrale per diagonalizzarne la matrice associata (la stessa associata a $\phi$). \\ \\
	
	\noindent
	Prendo una base $\mathcal{B} = \{\bar{v}_{1} \dots \bar{v}_{n}\}$ e definisco due vettori nello spazio
	\[
	\bar{x} = x_{1}\bar{v}_{1}+ \dots +x_{n}\bar{v}_{n}
	\]
	\[
	\bar{y} = y_{1}\bar{v}_{1}+ \dots +y_{n}\bar{v}_{n}
	\]
	Definisco ora un prodotto scalare:
	\[
	\bar{x} \cdot \bar{y} = x_{1}y_{1} + x_{2}y_{2} + \dots + x_{n}y_{n}
	\]
	Rispetto a questo prodotto scalare la nostra \textbf{base è ortonormale}. \\ \\
	
	
	\noindent
	Definisco ora l'endomorfismo $f:V\to V$ determinato univocamente da $f(\bar{v}_{i})$. Trovo le componenti di $f(\bar{v}_{i})$  facendo $f(\bar{v}_{i}) \cdot \bar{v}_{j}$ che abbiamo detto essere uguale a $\phi (\bar{v}_{i}, \bar{v}_{j})$. Si ha quindi:
	\[
	f(\bar{v}_{i})\cdot \bar{v}_{j} = \phi (\bar{v}_{i},\bar{v}_{j}) = \leftindex^{t}{X}AY
	\]

	Le componenti le esplicitiamo come:
	\[
	f(\bar{v}_{1})  = \phi(\bar{v}_{1},\bar{v}_{1}) \bar{v}_{1} + \dots + \phi(\bar{v}_{1},\bar{v}_{n})\bar{v}_{n}
	\]
	\[
	f(\bar{v}_{2})  = \phi(\bar{v}_{2},\bar{v}_{1}) \bar{v}_{1} + \dots + \phi(\bar{v}_{2},\bar{v}_{n})\bar{v}_{n}
	\]
	\[
	\vdots
	\]
\end{es}
\begin{es}{}
	Abbiamo quindi specificato $f$ in modo unico e vale
	\[
	f(\bar{x})\cdot \bar{y} = \phi(\bar{x},\bar{y}) \quad \forall \bar{x},\bar{y} 
	\]
	con
	\[
	M^\mathcal{B}(f) = \leftindex^{t}{A} = A \quad \Longrightarrow \quad f \text{ autoaggiunto} \quad \Longrightarrow \quad \exists \mathcal{B}' \text{ ortonormale di autovettori}
	\] \QEDB
\end{es}

\subsection{Forma normale}
	\begin{center}
	\fboxsep11pt
	\colorbox{myblue}{\begin{minipage}{5.75in}
			\begin{blues}{Definizione: forma normale}
			Diciamo  che una base $\mathcal{B}$ mette $\phi$ in forma normale se la matrice rappresentativa di $\phi$ è diagonale con solo $1$, $-1$ e $0$ sulla diagonale.
				
			\end{blues}
	\end{minipage}}       
\end{center}

\noindent
E' interessante notare che mettere in forma normale una $\phi$ definita positiva (quindi con solo uno sulla diagonale) vuole dire trovare una base ortonormale per il prodotto scalare.

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema di esistenza di una forma normale}
				Come per la forma canonica esiste sempre una base che mette $\phi$ in forma normale.
			\end{redes}
	\end{minipage}}        
\end{center}


\begin{es}{dimostrazione}
	Diciamo di partire da una base $\mathcal{B} = \left\{\bar{v_{1}},\bar{v}_{2},\dots,\bar{v}_{n}\right\}$ che mette $\phi$ in forma canonica, allora sappiamo che 
	\[
	\phi(\bar{v_{1}},\bar{v_{1}}) = \lambda_1
	\]
	\[
	\phi(\bar{v_{2}},\bar{v_{2}}) = \lambda_2
	\]
	\[
	\vdots
	\]
	\[
	\phi(\bar{v_{n}},\bar{v_{n}}) = \lambda_n
	\]
	e che i $\lambda_i$ sono gli elementi della diagonale. Il mio obiettivo è quello di trovare una base che metta solo $1$ e $-1$ al posto degli autovalori. Cerco quindi dei vettori tali che
	\[
	\phi(\mu\bar{ v_{j}},\mu\bar{ v_{j}}) = \pm 1
	\]
	Quindi dividiamo nei casi in cui gli autovalori sono positivi negativi o nulli:
	\[
	\lambda_i > 0 \qquad  \phi(\mu\bar{v_{i}},\mu\bar{v_{i}}) =  1 \quad \to \mu^2\phi(\bar{v_{i}},\bar{v_{i}}) =  1 \quad \to \phi(\bar{v_{i}},\bar{v_{i}}) =  \frac{1}{\mu^2}
	\]
	\[
	\frac{1}{\mu^2} = \lambda_i \quad \to \mu = \frac{1}{\sqrt{\lambda_i}}
	\]
	\[
	\lambda_j < 0 \qquad  \phi(\ni\bar{v_{j}},\ni\bar{v_{j}}) =  -1 \quad \to \ni^2\phi(\bar{v_{j}},\bar{v_{j}}) =  -1 \quad \to \phi(\bar{v_{j}},\bar{v_{j}}) =  \frac{-1}{\ni^2}
	\]

	\[
	\frac{-1}{\ni^2} = \lambda_i \quad \to \ni = \frac{1}{\sqrt{-\lambda_i}}
	\]
	\[
	\lambda_k = 0 \qquad \beta^2\phi(\bar{v}_{k},\bar{v}_{k}) = 0 \qquad \forall \beta \in \mathbb{R}
	\]
\end{es}
\begin{es}{}
	Abbiamo trovato quindi la base che mette $\phi$ in forma normale. Mettendo in ordine i vettori con prima quelli associati ad autovalori positivi, poi negativi e alla fine nulli:
	\[
	\mathcal{B}' = \left\{\frac{1}{\sqrt{\lambda_i}}\bar{v_{i}},\frac{1}{\sqrt{-\lambda_j}}\bar{v}_{j},\bar{v}_{k}\right\}
	\]
	\\
	\QEDB
\end{es}

\begin{center}
	\fboxsep11pt
	\colorbox{myred}{\begin{minipage}{5.75in}
			\begin{redes}{}
				\subsubsection{Teorema di Sylvester}
				Definita \textbf{segnatura} la mappa $(p,q)$ con $p$ numero di autovalori positivi e $q$ numero di autovalori negativi, il teorema afferma che matrici simili hanno stessa segnatura. 
			\end{redes}
	\end{minipage}}        
\end{center}
\begin{es}{dimostrazione}
	Data $\mathcal{B} = \left\{\bar{v}_{1},\dots,\bar{v}_{n}\right\}$ tale che 
	\[
	M^{\mathcal{B}}\phi = 
	\left[\begin{array}{ccc}
		I_{p} & & \\
		 & -I_{q} & \\
		  & & 0\\
	\end{array}\right]
	\]
	e in componenti rispetto a $\mathcal{B}$
	\[
	Q(\bar{x}) = x_{1}^2 + \dots + x_{p}^2 - x_{p+1}^2 - \dots - x_{p+q}^2 
	\]
	Data $\mathcal{B}' = \left\{\bar{v}'_{1},\dots,\bar{v}'_{n}\right\}$ tale che 
	\[
	M^{\mathcal{B}'}\phi = 
	\left[\begin{array}{ccc}
		I_{p'} & & \\
		& -I_{q'} & \\
		& & 0\\
	\end{array}\right]
	\]
	e in componenti rispetto a $\mathcal{B}$
	\[
	Q(\bar{x}) = (x')_{1}^2 + \dots + (x')_{p'}^2 - (x')_{p'+1}^2 - \dots - (x')_{p'+q'}^2 
	\]
	Abbiamo $p + q = p' + q' = \text{rk}\phi$. \\
	
	Supponiamo ora che $\mathbf{p > p'}$ e di conseguenza $q < q'$ e definiamo due sottospazi $W_{1}$ e $W_{2}$ rispettivamente di dimensioni $p$ e $q'$ (dal teorema mi aspetto che questi sottospazi siano supplementari). \\
	\[
	\text{dim}W_{1} + \text{dim}W_{2} = p + (n-p') \mathbf{> n}
	\]
	Quindi per Grassman si ha $\text{dim}(W_{1} \cap \text{dim}W_{2}) \geq 1$ ed esiste un vettore non nullo $\bar{x} \in W_{1} \cap W_{2}$.
	\\
	\QEDB
\end{es}

\end{document}



